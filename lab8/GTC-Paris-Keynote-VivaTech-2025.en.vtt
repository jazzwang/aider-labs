WEBVTT
Kind: captions
Language: en

00:00:50.720 --> 00:00:57.600
This is how intelligence is made.
A new kind of factory.&nbsp;

00:00:57.600 --> 00:01:03.840
Generator of tokens
The building blocks of AI&nbsp;

00:01:03.840 --> 00:01:10.640
Tokens have opened a new frontier
The first step into an extraordinary world.&nbsp;

00:01:10.640 --> 00:01:23.040
Where endless possibilities are born.
Tokens transform images into scientific data.&nbsp;

00:01:23.040 --> 00:01:30.080
Charting alien atmospheres.
And guiding the explorers of tomorrow.&nbsp;

00:01:32.560 --> 00:01:40.000
They probe the Earth's depths.
To seek out hidden danger.&nbsp;

00:01:40.000 --> 00:01:47.200
They turn potential
Into plenty&nbsp;

00:01:47.200 --> 00:01:58.080
And help us harvest our bounty.
Tokens see disease before it takes hold.&nbsp;

00:02:01.040 --> 00:02:09.120
Cure with precision
And learn&nbsp;

00:02:09.120 --> 00:02:18.720
What makes us tick?
Token connect the dots&nbsp;

00:02:18.720 --> 00:02:23.680
So we can protect our most noble creatures.&nbsp;

00:02:26.320 --> 00:02:38.240
Tokens decode the laws of physics.
To move us faster.&nbsp;

00:02:38.240 --> 00:02:49.040
And make our days more efficient.
Tokens don't just teach robots how to move,&nbsp;&nbsp;

00:02:49.040 --> 00:02:54.480
but to bring joy.
And comfort&nbsp;

00:02:54.480 --> 00:03:00.960
Mar. Hi.
Are you ready to see the Dr.? What's that?&nbsp;

00:03:00.960 --> 00:03:13.600
It's my enchanting tool.
Tokens help us move forward.&nbsp;

00:03:13.600 --> 00:03:21.600
One small step for man.
Becomes one giant leap for mankind.&nbsp;

00:03:22.880 --> 00:03:38.160
So we can boldly go
Where no one has gone before.&nbsp;

00:03:38.160 --> 00:03:46.640
And here
Is where it all begins.&nbsp;

00:03:58.960 --> 00:04:07.440
Welcome to the Stage NVIDIA&nbsp;
Founder and CEO Jensen Huang.&nbsp;

00:04:11.760 --> 00:04:26.640
Oh.
Bonjour.&nbsp;

00:04:26.640 --> 00:04:34.720
NVIDIA's first GTC in Paris.
This is incredible.&nbsp;

00:04:34.720 --> 00:04:36.880
Thank you for all the&nbsp;
partners who are here with us.&nbsp;

00:04:39.520 --> 00:04:42.560
We have so many people that&nbsp;
we work with over the years.&nbsp;

00:04:42.560 --> 00:04:46.240
In fact, we've been in Europe for a very&nbsp;
long time though. This is my first GTC&nbsp;&nbsp;

00:04:46.240 --> 00:04:54.400
Paris. I have a lot to tell you.
NVIDIA, once upon a time,&nbsp;&nbsp;

00:04:54.400 --> 00:05:02.880
wanted to create a new computing platform.
To do things that normal computers cannot.&nbsp;

00:05:02.880 --> 00:05:09.120
We accelerated the CPU, created a new type of&nbsp;
computing called accelerated computing, and one&nbsp;&nbsp;

00:05:09.120 --> 00:05:16.720
of our first applications was molecular dynamics.
We've come a long way since.&nbsp;

00:05:16.720 --> 00:05:22.400
So many different libraries. And in fact,&nbsp;
what makes accelerated computing special?&nbsp;

00:05:23.520 --> 00:05:28.960
It's not just a new processor&nbsp;
that you compile software to.&nbsp;

00:05:28.960 --> 00:05:35.600
You have to reformulate how you do computing.&nbsp;
You have to reformulate your algorithm and it&nbsp;&nbsp;

00:05:35.600 --> 00:05:40.720
turns out to be incredibly hard for&nbsp;
people to reformulate software and&nbsp;&nbsp;

00:05:40.720 --> 00:05:47.040
algorithms to be highly parallelized.
And so we created libraries to help.&nbsp;

00:05:47.040 --> 00:05:52.400
Each market, each domain of application.
Becomes accelerated. Each one of these&nbsp;&nbsp;

00:05:52.400 --> 00:05:59.360
libraries opens up new opportunities for the&nbsp;
developers and it opens up new opportunities&nbsp;&nbsp;

00:05:59.360 --> 00:06:06.640
for growth for us and our ecosystem partners.
Computational lithography. Probably the single&nbsp;&nbsp;

00:06:06.640 --> 00:06:12.960
most important application in semiconductor&nbsp;
design today, runs in a factory at TSMC.&nbsp;

00:06:12.960 --> 00:06:20.320
Samsung large semiconductor fabs. Before the&nbsp;
chip is made, it runs through an inverse physics&nbsp;&nbsp;

00:06:20.320 --> 00:06:25.520
algorithm called cuLitho, computational
lithography.&nbsp;

00:06:25.520 --> 00:06:31.520
Direct sparse solvers
Algebraic multi-grid solvers.&nbsp;

00:06:31.520 --> 00:06:38.080
CuOpt, we just opened sourced.&nbsp;
incredibly exciting Application Library.&nbsp;&nbsp;

00:06:38.080 --> 00:06:46.000
This library accelerates decision making
To optimize problems with millions of&nbsp;&nbsp;

00:06:46.000 --> 00:06:51.280
variables for millions of constraints&nbsp;
like traveling salespeople problems.&nbsp;

00:06:51.280 --> 00:06:56.640
Warp, a Pythonic
Framework for expressing&nbsp;&nbsp;

00:06:56.640 --> 00:07:07.120
geometry and physics solvers…really important.
cuDF cuML, Structured databases, DataFrames,&nbsp;&nbsp;

00:07:07.120 --> 00:07:13.600
classical machine learning algorithms
cuDf accelerates Spark with zero&nbsp;&nbsp;

00:07:13.600 --> 00:07:17.520
lines of code change.
cuML accelerates Sidekick&nbsp;&nbsp;

00:07:17.520 --> 00:07:22.800
learn with zero lines of code change.
Dynamo and CuDNN&nbsp;

00:07:22.800 --> 00:07:29.120
cuDNN is probably the single most important&nbsp;
library NVIDIA has ever created. It accelerates&nbsp;&nbsp;

00:07:29.120 --> 00:07:35.600
the primitives of deep neural networks. And&nbsp;
Dynamo is our brand new library that makes&nbsp;&nbsp;

00:07:35.600 --> 00:07:42.000
it possible to dispatch, orchestrate,&nbsp;
distribute extremely complex inference&nbsp;&nbsp;

00:07:42.000 --> 00:07:48.800
workloads across an entire AI factory.
cuEquivariance and cuTensor tensor&nbsp;&nbsp;

00:07:48.800 --> 00:07:53.360
contraction algorithms.
Equivariance is for neural&nbsp;&nbsp;

00:07:53.360 --> 00:08:00.800
networks that obey the laws of geometry, such as&nbsp;
proteins, molecules, Aerial, and Sionna. Really&nbsp;&nbsp;

00:08:00.800 --> 00:08:11.520
important framework to enable AI to run 6G.
Earth-2, our simulation environment for&nbsp;&nbsp;

00:08:11.520 --> 00:08:19.280
foundation models of weather and climate models.
Kilometer-squared, incredibly high resolution.&nbsp;

00:08:19.280 --> 00:08:26.640
MONAI, our framework for medical imaging,&nbsp;
incredibly popular. Parabricks is a solver&nbsp;&nbsp;

00:08:26.640 --> 00:08:33.600
for genomics analysis incredibly successful.
cuQuantum, CUDA-Q I'll talk about in just&nbsp;&nbsp;

00:08:33.600 --> 00:08:40.080
a second for quantum computing and
cuPyNumeric acceleration for NumPy and&nbsp;&nbsp;

00:08:40.080 --> 00:08:48.800
SciPy. As you can see, these are just a few of&nbsp;
the examples of libraries. There are 400 others.&nbsp;

00:08:48.800 --> 00:08:52.160
Each one of them accelerates&nbsp;
a domain of application.&nbsp;

00:08:52.160 --> 00:08:58.720
Each one of them opens up new opportunities.
Well, one of the most exciting.&nbsp;

00:08:58.720 --> 00:09:05.120
Most exciting is CUDA-Q.
CUDA-X is this suite of libraries.&nbsp;

00:09:05.120 --> 00:09:12.880
Library suite for accelerating applications and&nbsp;
algorithms on top of CUDA. We now have CUDA-Q.&nbsp;

00:09:12.880 --> 00:09:23.920
CUDA-Q is for quantum computing. For&nbsp;
quantum classical computing based on GPUs.&nbsp;

00:09:23.920 --> 00:09:30.320
We've been working on CUDA now for several years.
And today I can tell you there's an inflection&nbsp;&nbsp;

00:09:30.320 --> 00:09:35.120
point happening in quantum computing.
As you know.&nbsp;

00:09:35.120 --> 00:09:40.560
The first physical qubit was&nbsp;
demonstrated some nearly 30 years ago.&nbsp;

00:09:40.560 --> 00:09:44.640
An error correction algorithm&nbsp;
was invented in 1995.&nbsp;

00:09:44.640 --> 00:09:51.200
And in 2023, almost 30 years later.
The world's first logical&nbsp;&nbsp;

00:09:51.200 --> 00:09:57.520
qubit was demonstrated by Google.
Since then, a couple of years later, the number&nbsp;&nbsp;

00:09:57.520 --> 00:10:04.400
of logical qubits which is represented by a whole&nbsp;
lot of physical qubits with error correction,&nbsp;&nbsp;

00:10:04.400 --> 00:10:12.080
the number of logical qubits are starting to grow.
Well, just like Moore's Law would. I could totally&nbsp;&nbsp;

00:10:12.080 --> 00:10:17.920
expect ten times more logical qubits every&nbsp;
five years, 100 times more logical qubits&nbsp;&nbsp;

00:10:17.920 --> 00:10:23.840
every ten years. Those logical qubits would&nbsp;
become better error corrected, more robust,&nbsp;&nbsp;

00:10:23.840 --> 00:10:28.800
higher performance, more resilient, and&nbsp;
of course will continue to be scalable.&nbsp;

00:10:28.800 --> 00:10:33.680
Quantum computing is reaching an inflection&nbsp;
point. We've been working with quantum&nbsp;&nbsp;

00:10:33.680 --> 00:10:37.440
computing companies all over the&nbsp;
world in several different ways,&nbsp;&nbsp;

00:10:37.440 --> 00:10:44.640
but here in Europe there's a large community.
I saw Pascal last night. I saw Barcelona&nbsp;&nbsp;

00:10:44.640 --> 00:10:50.880
supercomputing last night. It is clear now.
We are within reach of being able to apply&nbsp;&nbsp;

00:10:50.880 --> 00:10:55.680
quantum computing, quantum classical&nbsp;
computing, in areas that can solve some&nbsp;&nbsp;

00:10:55.680 --> 00:11:01.680
interesting problems in the coming years.
This is a really exciting time. and&nbsp;&nbsp;

00:11:01.680 --> 00:11:06.240
so we've been working with all of the&nbsp;
supercomputing centers. It's very clear now.&nbsp;

00:11:06.240 --> 00:11:10.480
That over the next several years, or at&nbsp;
least the next generation of supercomputers,&nbsp;&nbsp;

00:11:10.480 --> 00:11:16.240
every single one of them will have a QPU&nbsp;
assigned and QPU connected to GPUs. The QPU&nbsp;&nbsp;

00:11:16.240 --> 00:11:20.480
will do quantum computing, of course.
And the GPUs would be used for&nbsp;&nbsp;

00:11:20.480 --> 00:11:23.840
pre processing for control.
Error correction, which would&nbsp;&nbsp;

00:11:23.840 --> 00:11:29.200
be intensely computationally intensive,&nbsp;
post processing and such. Between the two&nbsp;&nbsp;

00:11:29.200 --> 00:11:36.560
architectures, just as we accelerated the&nbsp;
CPU, now there's QPU working with the GPU&nbsp;&nbsp;

00:11:36.560 --> 00:11:45.360
to enable the next generation of computing.
Well, today we're announcing that our entire&nbsp;&nbsp;

00:11:45.360 --> 00:11:53.200
quantum algorithm stack is now accelerated on&nbsp;
Grace Blackwell 200 and the speed up is utterly&nbsp;&nbsp;

00:11:53.200 --> 00:11:59.440
incredible. We work with the Quantum computing&nbsp;
industry in several different ways. One way is&nbsp;&nbsp;

00:11:59.440 --> 00:12:08.000
using cuQuantum to simulate the qubits or simulate&nbsp;
the algorithms that runs on top of these quantum&nbsp;&nbsp;

00:12:08.000 --> 00:12:16.480
computers, essentially using a classical computer&nbsp;
to simulate or emulate a quantum computer.&nbsp;

00:12:16.480 --> 00:12:21.040
At the other extreme,&nbsp;
extremely important is CUDA-Q.&nbsp;

00:12:21.040 --> 00:12:27.200
Basically inventing a new CUDA that&nbsp;
extends CUDA into quantum classical so&nbsp;&nbsp;

00:12:27.200 --> 00:12:34.400
that applications that are developed on CUDA.
Can run before the quantum computer arrives&nbsp;&nbsp;

00:12:34.400 --> 00:12:41.120
in an emulated way or after the quantum&nbsp;
computer arrives in a collaborative way.&nbsp;

00:12:41.120 --> 00:12:45.760
A quantum classical&nbsp;
accelerated computing approach.&nbsp;

00:12:45.760 --> 00:12:50.720
And so today we're announcing CUDA&nbsp;
is available for Grace Blackwell.&nbsp;&nbsp;

00:12:50.720 --> 00:12:56.480
The ecosystem here is incredibly rich.
Of course, Europe is deep with science&nbsp;&nbsp;

00:12:56.480 --> 00:12:59.600
and deep with supercomputing&nbsp;
expertise and deep with heritage&nbsp;&nbsp;

00:12:59.600 --> 00:13:06.320
in this area. And it's not surprising to see.
quantum computing advance here. in the next&nbsp;&nbsp;

00:13:06.320 --> 00:13:11.600
several years, we're going to see a really&nbsp;
fantastic inflection point. So anyways, for&nbsp;&nbsp;

00:13:11.600 --> 00:13:17.600
all of the quantum industry that have been working&nbsp;
on this for three decades now, I congratulate you&nbsp;&nbsp;

00:13:17.600 --> 00:13:30.880
for just the incredible accomplishment&nbsp;
and the milestones today. Thank you.&nbsp;

00:13:30.880 --> 00:13:35.360
Let's talk about AI.
You might be surprised&nbsp;

00:13:35.360 --> 00:13:41.200
That I would I would be talking to&nbsp;
you about AI. the same, the same GPU.&nbsp;

00:13:41.200 --> 00:13:44.800
That
Ran and enabled all of these&nbsp;&nbsp;

00:13:44.800 --> 00:13:51.600
applications that I mentioned. That same GPU.
Enabled artificial intelligence to come&nbsp;&nbsp;

00:13:51.600 --> 00:13:57.040
to the world. Our first contact&nbsp;
was in 2012 just prior to that.&nbsp;

00:13:57.040 --> 00:14:01.440
Working with developers on a new&nbsp;
type algorithm called deep learning.&nbsp;

00:14:01.440 --> 00:14:12.000
It enabled the AlexNet big bang of AI in 2012.
In the last 15 years or so AI has progressed&nbsp;&nbsp;

00:14:12.000 --> 00:14:16.800
incredibly fast. The first&nbsp;
wave of AI was perception.&nbsp;

00:14:16.800 --> 00:14:22.160
For computers to recognize information.
Understand it. The second wave,&nbsp;&nbsp;

00:14:22.160 --> 00:14:27.920
which most of us were talking about.
The last five years or so was Generative AI.&nbsp;&nbsp;

00:14:27.920 --> 00:14:34.400
It's multimodal, meaning that an AI was&nbsp;
able to learn both images and Language.&nbsp;

00:14:34.400 --> 00:14:38.960
Therefore, you could prompt with&nbsp;
Language and it could generate images.&nbsp;

00:14:38.960 --> 00:14:45.360
The ability of AI to be multimodal as&nbsp;
well as able to translate and generate&nbsp;&nbsp;

00:14:45.360 --> 00:14:51.760
content enabled the Generative AI revolution.
Generative AI, the ability to generate content&nbsp;&nbsp;

00:14:51.760 --> 00:14:58.960
is fundamentally vital. For us to be productive
Well, we've got a new, we are starting a new&nbsp;&nbsp;

00:14:58.960 --> 00:15:06.400
wave of AI. and this last couple of years&nbsp;
we've seen enormous progress in AI's ability.&nbsp;

00:15:06.400 --> 00:15:12.000
Fundamentally, intelligence&nbsp;
is about understanding.&nbsp;

00:15:12.000 --> 00:15:15.680
Perception
Reasoning&nbsp;

00:15:15.680 --> 00:15:22.320
Planning a task, how to solve a problem, and&nbsp;
then executing the task, perception, reasoning,&nbsp;&nbsp;

00:15:22.320 --> 00:15:29.200
planning. the fundamental cycles of intelligence.&nbsp;
It allows us to apply some previously learned&nbsp;&nbsp;

00:15:29.200 --> 00:15:36.800
rules to solve problems we've never seen before.
That's why intelligent people are considered&nbsp;&nbsp;

00:15:36.800 --> 00:15:43.120
intelligent to be able to take a complicated&nbsp;
problem, break it down step by step.&nbsp;

00:15:43.120 --> 00:15:47.200
Reason about how to solve the&nbsp;
problem, maybe do research.&nbsp;

00:15:47.200 --> 00:15:52.240
Maybe go learn some new information.
Get some help, use tools, and solve&nbsp;&nbsp;

00:15:52.240 --> 00:15:57.280
problems step by step. Well, the words&nbsp;
that I just described are fundamentally&nbsp;&nbsp;

00:15:57.280 --> 00:16:02.640
possible today with what is called Agentic&nbsp;
AI, and I'll show you more in just a second.&nbsp;

00:16:02.640 --> 00:16:08.320
In the physical implementation of that,&nbsp;
the embodiment of that Agentic AI.&nbsp;

00:16:09.360 --> 00:16:14.560
Now the Generative capability is generating&nbsp;
motion. Instead of generating videos and&nbsp;&nbsp;

00:16:14.560 --> 00:16:21.200
generating images or generating text, this AI&nbsp;
generates locomotion, the ability to walk or&nbsp;&nbsp;

00:16:21.200 --> 00:16:26.320
reach out and grab something, use tools.
The ability for AI to be embodied in&nbsp;&nbsp;

00:16:26.320 --> 00:16:33.360
a physical form is basically robotics.
These capabilities. the fundamental technology&nbsp;

00:16:33.360 --> 00:16:42.720
To enable agents, which are basically information&nbsp;
robots and embodied AI, physical robots. These two&nbsp;&nbsp;

00:16:42.720 --> 00:16:50.320
fundamental capabilities are now upon us. really,&nbsp;
really exciting times for AI. But it all started.&nbsp;

00:16:50.320 --> 00:16:55.120
It all started with GeForce.
And GeForce brought computer&nbsp;&nbsp;

00:16:55.120 --> 00:17:00.400
graphics. This is the first accelerated&nbsp;
computing application we had ever worked on.&nbsp;

00:17:00.400 --> 00:17:04.960
And it's incredible how far&nbsp;
computer graphics has come.&nbsp;

00:17:04.960 --> 00:17:10.960
GeForce brought CUDA to the world, which&nbsp;
enabled machine learning researchers and&nbsp;&nbsp;

00:17:10.960 --> 00:17:17.920
AI researchers to advance deep learning.
Then deep learning revolutionized computer&nbsp;&nbsp;

00:17:17.920 --> 00:17:24.160
graphics and made it possible for us to&nbsp;
bring computer graphics to a whole new level.&nbsp;

00:17:24.160 --> 00:17:27.760
Everything I'm going to show you today.
Everything I'm going to you today,&nbsp;&nbsp;

00:17:27.760 --> 00:17:31.600
I'm going to give you a preview of what I'm going&nbsp;
to show you. But everything I'm going to show you&nbsp;&nbsp;

00:17:31.600 --> 00:17:40.560
today is computer simulation, not animation.
It's photon simulation, physics simulation,&nbsp;&nbsp;

00:17:40.560 --> 00:17:44.240
particle simulations.
Everything is fundamentally&nbsp;&nbsp;

00:17:44.240 --> 00:17:50.640
simulation, not animation, not art. It just&nbsp;
looks incredibly beautiful because it turns&nbsp;&nbsp;

00:17:50.640 --> 00:18:13.760
out the world is beautiful and it turns out&nbsp;
math is beautiful. So let's take a look.

00:20:04.720 --> 00:20:15.040
What do you think?
Numbers in action. Numbers in&nbsp;&nbsp;

00:20:15.040 --> 00:20:20.320
action. That's essentially what simulations are&nbsp;
and it's just incredibly beautiful to look at.&nbsp;&nbsp;

00:20:20.320 --> 00:20:24.880
But because of the scale.
And the speed by which we&nbsp;&nbsp;

00:20:24.880 --> 00:20:29.840
can now simulate almost everything.
We can turn everything into a digital&nbsp;&nbsp;

00:20:29.840 --> 00:20:35.680
twin. And because everything can be a digital&nbsp;
twin, it could be designed, planned, optimized,&nbsp;&nbsp;

00:20:35.680 --> 00:20:42.320
and operated completely digitally before we&nbsp;
put it into the physical world. The idea that&nbsp;&nbsp;

00:20:42.320 --> 00:20:48.320
we would build everything in software is now upon&nbsp;
us. Everything physical will be built digitally.&nbsp;&nbsp;

00:20:48.320 --> 00:20:52.240
Everything that's built magnificently&nbsp;
will be built digitally. Everything&nbsp;&nbsp;

00:20:52.240 --> 00:20:57.520
that's operated at gigantic scale will be first&nbsp;
built digitally, and there will be digital twins&nbsp;&nbsp;

00:20:57.520 --> 00:21:01.600
that operate it. And so today we're going&nbsp;
to talk a lot about digital twins. Well,&nbsp;&nbsp;

00:21:01.600 --> 00:21:08.480
what started out as a GeForce graphics&nbsp;
card…Anybody in here know what a GeForce is?&nbsp;

00:21:08.480 --> 00:21:13.920
Okay.
Right. Well, what started out as GeForce?&nbsp;

00:21:13.920 --> 00:21:25.360
Looks like this now this is the new GeForce. It is&nbsp;
two tons, two and a half tons, 1.2 million parts.&nbsp;

00:21:25.360 --> 00:21:33.040
About $3 million
120 KW.&nbsp;

00:21:33.040 --> 00:21:37.120
Manufactured in 150 factories.&nbsp;

00:21:37.120 --> 00:21:41.040
200 technology partners&nbsp;
working with us to do this.&nbsp;

00:21:41.040 --> 00:21:48.480
Probably something along the lines of $40&nbsp;
billion in R&amp;D budget in order to create&nbsp;&nbsp;

00:21:48.480 --> 00:21:56.400
what is GB200 and now moving to GB300. It is&nbsp;
completely in production. And this machine was&nbsp;&nbsp;

00:21:56.400 --> 00:22:03.280
designed to be a thinking machine. A thinking&nbsp;
machine in the sense that it reasons, it plans.&nbsp;

00:22:03.280 --> 00:22:08.080
It spends a lot of time talking to itself.
Just like you do.&nbsp;

00:22:08.800 --> 00:22:16.080
We spend most of our time generating words for&nbsp;
our own mind, generating images for our own mind&nbsp;&nbsp;

00:22:16.080 --> 00:22:22.240
before we produce it. And so the thinking machine&nbsp;
is really architecturally what Grace Blackwell was&nbsp;&nbsp;

00:22:22.240 --> 00:22:29.040
designed to do. It was designed to be one giant&nbsp;
GPU. I compared it to GeForce for a good reason.&nbsp;

00:22:29.040 --> 00:22:39.360
GeForce is one GPU, so is GB200. It is one giant&nbsp;
virtual GPU. Now we had to disaggregate it into&nbsp;&nbsp;

00:22:39.360 --> 00:22:45.280
a whole bunch of components, create a bunch of&nbsp;
new networking technology and SerDes technology,&nbsp;&nbsp;

00:22:45.280 --> 00:22:52.880
incredibly low, low power, high energy efficiency&nbsp;
interconnects to connect all of these chips and&nbsp;

00:22:52.880 --> 00:23:01.120
Systems together into one virtual&nbsp;
GPU. This is the hopper version.&nbsp;&nbsp;

00:23:01.120 --> 00:23:08.240
This is the world famous hopper system.
Eight GPUs connected together on NVLink.&nbsp;

00:23:08.240 --> 00:23:13.440
What's not shown here is a CPU&nbsp;
tray, a CPU tray with dual CPU&nbsp;&nbsp;

00:23:13.440 --> 00:23:22.880
and system memory that sits on top together.
This represents one node of an AI supercomputer&nbsp;

00:23:22.880 --> 00:23:27.840
About half a million dollars. This is&nbsp;
the hopper system. This is the system&nbsp;&nbsp;

00:23:27.840 --> 00:23:36.560
that really put us on the map of AI and It&nbsp;
was under allocation for a very long time.&nbsp;

00:23:36.560 --> 00:23:42.800
Because the market took off so quickly. But this&nbsp;
is the famous Hopper system. Well, this entire&nbsp;&nbsp;

00:23:42.800 --> 00:23:52.000
system, including the CPU, is replaced by this.
Great Blackwell node. This is one compute tray.&nbsp;

00:23:52.000 --> 00:23:57.520
Right here we'll replace that entire&nbsp;
system. It is fully liquid cooled.&nbsp;

00:23:57.520 --> 00:24:01.760
And the CPUs are integrated directly&nbsp;
connected to the GPUs. So you could&nbsp;&nbsp;

00:24:01.760 --> 00:24:08.800
see it here, two CPUs, four GPUs it is&nbsp;
more performant than that entire system.&nbsp;

00:24:08.800 --> 00:24:14.080
But what's amazing is this: we wanted to connect&nbsp;
a whole bunch of these systems together. How would&nbsp;&nbsp;

00:24:14.080 --> 00:24:20.800
you connect all of these together was really hard&nbsp;
for us to imagine. So we disaggregated it. What we&nbsp;&nbsp;

00:24:20.800 --> 00:24:27.600
did was we took that entire motherboard.&nbsp;
We disaggregated into this and this.&nbsp;

00:24:27.600 --> 00:24:34.720
This is the revolutionary NVLink system.
Scaling out computing is not that hard.&nbsp;&nbsp;

00:24:34.720 --> 00:24:42.960
Just connect more CPUs with Ethernet. Scaling&nbsp;
out is not hard. Scaling up is incredibly hard.&nbsp;

00:24:42.960 --> 00:24:46.320
You can only build as large of&nbsp;
a computer as you can build.&nbsp;

00:24:46.320 --> 00:24:53.200
The amount of technology and electronics that&nbsp;
you could fit into one memory Model is incredibly&nbsp;&nbsp;

00:24:53.200 --> 00:24:58.800
hard to do. And so what we decided to do was we&nbsp;
create a new interconnect called NVLink. NVLink&nbsp;&nbsp;

00:24:58.800 --> 00:25:05.280
is a memory semantics interconnect.&nbsp;
It's a compute fabric, not a network.&nbsp;

00:25:05.280 --> 00:25:10.160
It directly connects to the CPU of&nbsp;
all of these different NVLink systems&nbsp;&nbsp;

00:25:10.160 --> 00:25:20.080
compute nodes. This is the switch.
Nine of these stand on top. Nine of&nbsp;&nbsp;

00:25:20.080 --> 00:25:25.760
it sits on the bottom. In the middle are&nbsp;
the NVLink switches and what connects it&nbsp;&nbsp;

00:25:25.760 --> 00:25:34.640
together is this miracle.
This is the NVLink spine.&nbsp;

00:25:34.640 --> 00:25:40.240
This is 100% copper.
Copper coax. It directly&nbsp;&nbsp;

00:25:40.240 --> 00:25:46.720
connects all of the NVLink chips to all&nbsp;
of the GPUs directly connected over.&nbsp;

00:25:46.720 --> 00:25:57.120
This entire spine so that every single one of&nbsp;
the 144 Blackwell dies in 72 different packages&nbsp;&nbsp;

00:25:57.120 --> 00:26:04.800
are talking to each other at the same time.
Without blocking all across this NVLink spine,&nbsp;&nbsp;

00:26:04.800 --> 00:26:10.080
the bandwidth of this is about&nbsp;
130 terabytes per second.&nbsp;

00:26:10.080 --> 00:26:20.960
130. I know.
No, wait for it. wait for it.&nbsp;

00:26:20.960 --> 00:26:28.640
130 TBps if it's in bits.
130 TBps&nbsp;

00:26:29.520 --> 00:26:35.280
It is more than the data rate of the&nbsp;
peak traffic of the world's entire&nbsp;&nbsp;

00:26:35.280 --> 00:26:47.680
internet traffic, on this backplane. Yeah.
So this is how you shrink the internet into&nbsp;&nbsp;

00:26:47.680 --> 00:26:51.600
60 pounds.
NVLink&nbsp;

00:26:51.600 --> 00:26:59.120
And so we did all that. We did all that&nbsp;
because the way you think about computers&nbsp;&nbsp;

00:26:59.120 --> 00:27:02.800
is going to be fundamentally different in the&nbsp;
future and I'll spend more time on this but&nbsp;&nbsp;

00:27:02.800 --> 00:27:11.760
it was designed to give Blackwell a giant leap&nbsp;
above hopper. Remember Moore's Law semiconductor&nbsp;&nbsp;

00:27:11.760 --> 00:27:18.160
physics is only giving you about two times&nbsp;
more performance every three to five years.&nbsp;

00:27:18.160 --> 00:27:24.400
How could we achieve 30, 40 times more&nbsp;
performance in just one generation? And&nbsp;&nbsp;

00:27:24.400 --> 00:27:30.880
we need a 30, 40 times more performance because&nbsp;
the reasoning models are talking to themselves.&nbsp;

00:27:30.880 --> 00:27:38.720
Instead of one-shot ChatGPT, it's now a reasoning&nbsp;
model and it generates a ton more tokens. When&nbsp;&nbsp;

00:27:38.720 --> 00:27:43.840
you're thinking to yourself, you're breaking&nbsp;
the problem down step by step. You're reasoning,&nbsp;&nbsp;

00:27:43.840 --> 00:27:47.360
you're trying a whole bunch of different&nbsp;
paths. Maybe it's chain of thoughts,&nbsp;&nbsp;

00:27:47.360 --> 00:27:53.040
maybe it's tree of thoughts. Best of end, it's&nbsp;
reflecting on its own answers. You've probably&nbsp;&nbsp;

00:27:53.040 --> 00:27:58.960
seen these research models reflecting on the&nbsp;
answer, saying, is this a good answer? Can you do&nbsp;&nbsp;

00:27:58.960 --> 00:28:03.360
better than that? And they the, oh yeah, I can do&nbsp;
better than that goes back and thinks some more.&nbsp;&nbsp;

00:28:03.360 --> 00:28:11.040
And so those thinking models, reasoning models&nbsp;
achieve Incredible performance, but it requires&nbsp;&nbsp;

00:28:11.040 --> 00:28:20.800
a lot more computational capability. And the net&nbsp;
result? NVLink 72 with Blackwell’s architecture&nbsp;&nbsp;

00:28:20.800 --> 00:28:27.280
resulted in a giant leap in performance.
The way to read this is the x axis is how fast&nbsp;&nbsp;

00:28:27.280 --> 00:28:34.640
it's thinking. The y axis is how much the factory&nbsp;
can output supporting a whole bunch of users at&nbsp;&nbsp;

00:28:34.640 --> 00:28:40.160
one time. And so you want the throughput of the&nbsp;
factory to be as high as possible so you could&nbsp;&nbsp;

00:28:40.160 --> 00:28:46.240
support as many people as possible so that the&nbsp;
revenues of your factory is as high as possible.&nbsp;

00:28:46.240 --> 00:28:54.880
You want this axis to be as large as possible&nbsp;
because the AI is smarter here than it is here.&nbsp;&nbsp;

00:28:55.440 --> 00:29:00.400
The faster it can think, the more it can think&nbsp;
before it answers your question. And so this&nbsp;&nbsp;

00:29:00.400 --> 00:29:07.360
has to do with the ASP of the average selling&nbsp;
price of the tokens and this has to do with the&nbsp;&nbsp;

00:29:07.360 --> 00:29:14.640
throughput of the factories. These two combined&nbsp;
in that corner is the revenues of the factory.&nbsp;

00:29:14.640 --> 00:29:23.120
This factory based on Blackwell can generate a&nbsp;
ton more revenues as a result of the architecture.&nbsp;

00:29:23.120 --> 00:29:28.160
It is such an incredible thing what we built.
We made a movie for you just to give you a&nbsp;&nbsp;

00:29:28.160 --> 00:29:33.360
sense of the enormity of the engineering&nbsp;
that went into building Grace Blackwell,&nbsp;&nbsp;

00:29:33.360 --> 00:29:36.760
take a look.
[video]&nbsp;

00:29:38.080 --> 00:29:48.640
Blackwell is an engineering marvel.
It begins as a blank silicon wafer.&nbsp;

00:29:48.640 --> 00:29:52.480
Hundreds of chip processing&nbsp;
and ultraviolet lithography&nbsp;&nbsp;

00:29:52.480 --> 00:29:59.600
steps build up each of the two hundred billion&nbsp;
transistors layer by layer on a twelve -wafer.&nbsp;

00:30:00.160 --> 00:30:05.680
The wafer is scribed into individual&nbsp;
Blackwell dies, tested and sorted,&nbsp;&nbsp;

00:30:05.680 --> 00:30:11.920
separating the good dies to move forward.
The chip on wafer on substrate process&nbsp;&nbsp;

00:30:11.920 --> 00:30:22.640
attaches 32 Blackwell dies and 128 HBM&nbsp;
stacks on a custom silicon interposer wafer.&nbsp;

00:30:22.640 --> 00:30:28.800
Metal interconnect traces are etched directly&nbsp;
into it, connecting Blackwell GPUs and HBM&nbsp;&nbsp;

00:30:28.800 --> 00:30:34.720
stacks into each system and package&nbsp;
unit, locking everything into place.&nbsp;

00:30:34.720 --> 00:30:39.200
Then the assembly is baked, molded,&nbsp;
and cured, creating the Blackwell&nbsp;&nbsp;

00:30:39.200 --> 00:30:42.800
B200 superchip.
Each Blackwell&nbsp;&nbsp;

00:30:42.800 --> 00:30:51.520
is stress-tested in ovens at 125 °C.
And pushed to its limits for several hours.&nbsp;

00:30:54.000 --> 00:30:57.920
Robots work around the clock&nbsp;
to pick and place over 10,000&nbsp;&nbsp;

00:30:57.920 --> 00:31:06.480
components onto the Grace Blackwell PCB.
Meanwhile, custom liquid cooling copper&nbsp;&nbsp;

00:31:06.480 --> 00:31:17.200
blocks are prepared to keep the&nbsp;
chips at optimal temperatures.&nbsp;

00:31:17.200 --> 00:31:23.920
At another facility, ConnectX-7 super NICs are&nbsp;
built to enable scale-out communications and&nbsp;&nbsp;

00:31:23.920 --> 00:31:31.600
BlueField-3 DPUs to offload and accelerate&nbsp;
networking, storage, and security tasks.&nbsp;

00:31:31.600 --> 00:31:41.040
All these parts converge to be carefully&nbsp;
integrated into GB200 compute trays.&nbsp;

00:31:44.800 --> 00:31:50.320
NVLink is the breakthrough high speed link&nbsp;
that NVIDIA invented to connect multiple&nbsp;&nbsp;

00:31:50.320 --> 00:31:58.800
GPUs and scale up into a massive virtual GPU.
The NVLink switch tray is constructed with NVLink&nbsp;&nbsp;

00:31:58.800 --> 00:32:08.480
switch chips providing 14.4 TBps of all-to-all&nbsp;
bandwidth. NVLink spines form a custom blind mated&nbsp;&nbsp;

00:32:08.480 --> 00:32:17.520
backplane with 5,000 copper cables connecting all&nbsp;
72 Blackwells or 144 GPU dies into one giant GPU&nbsp;&nbsp;

00:32:17.520 --> 00:32:24.960
delivering 130 TBps of all-to-all bandwidth,&nbsp;
more than the global internet's peak traffic.&nbsp;

00:32:24.960 --> 00:32:30.080
From around the world, parts arrive to&nbsp;
be assembled by skilled technicians into&nbsp;&nbsp;

00:32:30.080 --> 00:32:45.760
a rack-scale AI supercomputer.
In total 1.2 million components,&nbsp;&nbsp;

00:32:45.760 --> 00:32:54.640
2 miles of copper cable, 130 trillion&nbsp;
transistors, weighing nearly two tons.&nbsp;

00:32:54.640 --> 00:32:58.880
Blackwell is more than a technological&nbsp;
wonder. It's a testament to the power&nbsp;&nbsp;

00:32:58.880 --> 00:33:03.840
of global collaboration and innovation,&nbsp;
fueling the discoveries and solutions&nbsp;&nbsp;

00:33:03.840 --> 00:33:10.320
that will shape our future everywhere.
We are driven to enable the geniuses of&nbsp;&nbsp;

00:33:10.320 --> 00:33:25.760
our time to do their life’s work and we can't&nbsp;
wait to see the breakthroughs you deliver.

00:33:25.760 --> 00:33:31.280
Grace Blackwell Systems
All in production. It is really a miracle.&nbsp;&nbsp;

00:33:31.280 --> 00:33:35.760
It's a miracle from a technology perspective.&nbsp;
But the supply chain that came together to build&nbsp;&nbsp;

00:33:35.760 --> 00:33:44.880
these GB200 systems, two tons each, we're&nbsp;
producing them now, 1,000 systems a week.&nbsp;

00:33:44.880 --> 00:33:49.520
No one has ever produced mass produced&nbsp;
supercomputers at this scale before.&nbsp;

00:33:49.520 --> 00:33:56.880
Each one of these racks is essentially&nbsp;
an entire supercomputer. Only in 2018,&nbsp;&nbsp;

00:33:56.880 --> 00:34:05.040
the largest Volta system, the Sierra supercomputer&nbsp;
in 2018 is less performant than one of these racks&nbsp;&nbsp;

00:34:05.040 --> 00:34:14.560
and that system was 10 MW. This is 100 KW. So the&nbsp;
difference generationally between 2018 and now,&nbsp;&nbsp;

00:34:14.560 --> 00:34:17.840
we've really taken supercomputing,&nbsp;
AI supercomputing to a whole new&nbsp;&nbsp;

00:34:17.840 --> 00:34:24.000
level and we're now producing these machinery at&nbsp;
enormous scales. And this is just the beginning.&nbsp;

00:34:24.880 --> 00:34:29.200
In fact, what you've seen is just one system,&nbsp;&nbsp;

00:34:29.200 --> 00:34:34.640
Grace Blackwell. The entire world is talking&nbsp;
of this one system, clamoring for it to get&nbsp;&nbsp;

00:34:34.640 --> 00:34:40.000
deployed into the world's data centers for&nbsp;
training and inferencing and Generative AI.

00:34:40.000 --> 00:34:47.040
However, not everybody and not every data&nbsp;
center handle these liquid cooled systems.&nbsp;&nbsp;

00:34:47.040 --> 00:34:54.320
Some data centers require enterprise stacks,&nbsp;
the ability to run Linux, Red Hat, or or VMware.&nbsp;&nbsp;

00:34:54.320 --> 00:35:02.800
Storage systems from Dell, Emc, Hitachi, Netapp,&nbsp;
Vast, Weka, so many different storage systems,&nbsp;&nbsp;

00:35:02.800 --> 00:35:09.040
so many different IT systems. And the management&nbsp;
of those has to be done in a way that's consistent&nbsp;&nbsp;

00:35:09.040 --> 00:35:15.840
with traditional IT systems. We have so many new&nbsp;
computers to ramp into production and I'm so happy&nbsp;&nbsp;

00:35:15.840 --> 00:35:22.160
to tell you that every single one of these are&nbsp;
now in production. You haven't seen them yet.&nbsp;&nbsp;

00:35:22.160 --> 00:35:28.160
They're all flying off the shelves, flying off&nbsp;
the ramps, the manufacturing lines starting here.&nbsp;

00:35:28.160 --> 00:35:36.320
DGX Spark enables you to have essentially&nbsp;
the Grace Blackwell system on your desktop&nbsp;&nbsp;

00:35:36.320 --> 00:35:43.280
in the case of Spark desktop, in the case of DGX&nbsp;
station desk side. This way you don't have to sit&nbsp;&nbsp;

00:35:43.280 --> 00:35:47.840
on a supercomputer while you're developing your&nbsp;
software, while you're developing your AI, but&nbsp;&nbsp;

00:35:47.840 --> 00:35:55.280
you want the architecture to be exactly the same.
These systems are identical from an architecture&nbsp;&nbsp;

00:35:55.280 --> 00:36:00.160
perspective. From a software developer&nbsp;
perspective, it looks exactly the same.&nbsp;&nbsp;

00:36:00.160 --> 00:36:05.440
The only difference is scale and speed.&nbsp;
And then on this side are all the X86&nbsp;&nbsp;

00:36:05.440 --> 00:36:12.160
systems. The world's IT organizations still&nbsp;
prefer X86 and appreciate X86 wherever they&nbsp;&nbsp;

00:36:12.160 --> 00:36:16.640
can take advantage of the most advanced AI&nbsp;
native systems. They do where they can't&nbsp;&nbsp;

00:36:16.640 --> 00:36:23.520
and they want to integrate into the enterprise IT&nbsp;
systems, we now offer them the ability to do so.&nbsp;

00:36:23.520 --> 00:36:28.560
One of the most important systems and it has&nbsp;
taken us the longest to build because of the&nbsp;&nbsp;

00:36:28.560 --> 00:36:32.480
software and the architecture is&nbsp;
so complicated is how to bring.&nbsp;

00:36:32.480 --> 00:36:39.120
The AI native architecture and infuse it&nbsp;
into the traditional enterprise IT system.&nbsp;

00:36:39.120 --> 00:36:45.520
This is our brand new RTX Pro&nbsp;
server. This is an incredible system.&nbsp;

00:36:45.520 --> 00:36:52.720
The motherboard is completely redesigned.
Ladies and gentlemen, Janine Paul.&nbsp;

00:36:59.920 --> 00:37:05.040
This motherboard looks so simple.
And yet on top of this motherboard&nbsp;&nbsp;

00:37:05.040 --> 00:37:14.160
are eight super NIC switches that connect eight&nbsp;
GPUs across a 200 Gbps state of the art networking&nbsp;&nbsp;

00:37:14.160 --> 00:37:22.960
chip that then connects eight of these GPUs and&nbsp;
these Blackwell RTX Pro 6000 GPUs, brand new,&nbsp;&nbsp;

00:37:22.960 --> 00:37:26.480
just entered into production.
Eight of these go into a&nbsp;&nbsp;

00:37:26.480 --> 00:37:31.440
server. Now what makes it special?
This server is the only server in&nbsp;&nbsp;

00:37:31.440 --> 00:37:38.800
the world that runs everything the world has ever&nbsp;
written and everything NVIDIA has ever developed.&nbsp;

00:37:38.800 --> 00:37:48.480
It runs AI, Omniverse, RTX for video&nbsp;
games. It runs Windows, it runs Linux,&nbsp;&nbsp;

00:37:48.480 --> 00:37:55.440
runs Kubernetes, it runs Kubernetes and VMware.
It runs basically everything. If you want to&nbsp;&nbsp;

00:37:55.440 --> 00:38:01.440
stream Windows desktop from a computer to your&nbsp;
to your remote device, no problem. If you want&nbsp;&nbsp;

00:38:01.440 --> 00:38:06.560
to stream Omniverse, no problem. If you want&nbsp;
to run your robotics stack, no problem. Just&nbsp;&nbsp;

00:38:06.560 --> 00:38:13.760
the QA of this particular machine is insane. The&nbsp;
applications that it runs are basically universal.&nbsp;&nbsp;

00:38:13.760 --> 00:38:20.480
Everything the world's ever developed should&nbsp;
run on here, including If you're a video gamer,&nbsp;&nbsp;

00:38:20.480 --> 00:38:28.960
including crisis.
And so if you can&nbsp;&nbsp;

00:38:28.960 --> 00:38:35.360
run crisis, you can run anything.
Okay, this is the RTX Pro Server&nbsp;&nbsp;

00:38:35.360 --> 00:38:42.800
brand new enterprise system.
So something is changing.&nbsp;

00:38:43.760 --> 00:38:47.760
We know that AI is incredibly&nbsp;
important technology.&nbsp;

00:38:47.760 --> 00:38:55.040
We know for a fact now that AI is software that&nbsp;
could revolutionize, transform every industry.&nbsp;

00:38:55.040 --> 00:38:59.280
It can do these amazing things.
That we know&nbsp;

00:38:59.280 --> 00:39:03.440
We also know that the way you&nbsp;
process AI is fundamentally&nbsp;&nbsp;

00:39:03.440 --> 00:39:07.440
different than the way we used to&nbsp;
process software written by hand.&nbsp;

00:39:07.440 --> 00:39:12.000
Machine learning software is developed&nbsp;
differently and it runs differently.&nbsp;

00:39:12.800 --> 00:39:17.280
The architecture of the systems, the architecture&nbsp;
of the software are completely different. The way&nbsp;&nbsp;

00:39:17.280 --> 00:39:23.840
the networking works is completely different.&nbsp;
The way it acts is completely different.&nbsp;

00:39:23.840 --> 00:39:29.760
So we know that the technology can do&nbsp;
different things, incredible things.&nbsp;

00:39:29.760 --> 00:39:33.760
It's intelligent. We also know that&nbsp;
it's developed in a fundamentally&nbsp;&nbsp;

00:39:33.760 --> 00:39:38.560
different way and needs new computers.
The thing that's really interesting,&nbsp;&nbsp;

00:39:38.560 --> 00:39:44.240
is what does this all mean, to&nbsp;
countries, to companies, to society.&nbsp;

00:39:44.240 --> 00:39:48.880
And this, this is an observation that&nbsp;
we made almost a decade ago that now&nbsp;&nbsp;

00:39:48.880 --> 00:39:55.840
everyone is awakening to, that in fact these&nbsp;
AI data centers are not data centers at all.&nbsp;

00:39:55.840 --> 00:39:58.880
They're not data centers in&nbsp;
the classical sense of a data&nbsp;&nbsp;

00:39:58.880 --> 00:40:06.160
center storing your files that you retrieve.
These data centers are not storing our files.&nbsp;&nbsp;

00:40:06.160 --> 00:40:14.720
It has one job and one job only: to produce&nbsp;
intelligent tokens, the generation of AI.&nbsp;

00:40:14.720 --> 00:40:21.280
These factories of AI look like&nbsp;
data centers in the sense that&nbsp;&nbsp;

00:40:21.280 --> 00:40:25.760
they have a lot of computers inside.
But that's where everything breaks down.&nbsp;

00:40:25.760 --> 00:40:31.920
How it's designed, the scale at&nbsp;
which its manufactured or scaled,&nbsp;&nbsp;

00:40:31.920 --> 00:40:37.680
designed and built and how it's used.
And how it's orchestrated and&nbsp;&nbsp;

00:40:37.680 --> 00:40:41.440
provisioned, operated.
How you think about it?&nbsp;

00:40:41.440 --> 00:40:50.800
For example, nobody really thinks about their&nbsp;
data center as a revenue generating facility.&nbsp;

00:40:50.800 --> 00:40:54.960
I said something that everybody&nbsp;
goes, yeah, I think you're right.&nbsp;

00:40:55.840 --> 00:41:00.080
Nobody ever thinks about a data center as a&nbsp;
revenue-generating facility, but they think&nbsp;&nbsp;

00:41:00.080 --> 00:41:06.160
of their factories, their car factories, as&nbsp;
revenue-generating facilities. And they can't&nbsp;&nbsp;

00:41:06.160 --> 00:41:11.680
wait to build another factory because whenever&nbsp;
you build a factory, revenue grows shortly&nbsp;&nbsp;

00:41:11.680 --> 00:41:20.160
after you could build more things for more people.
Those ideas are exactly the same ideas in these AI&nbsp;&nbsp;

00:41:20.160 --> 00:41:26.720
factories. They are revenue-generating facilities&nbsp;
and they are designed to manufacture tokens.&nbsp;

00:41:26.720 --> 00:41:34.320
And these tokens can be reformulated into&nbsp;
productive intelligence for so many industries&nbsp;&nbsp;

00:41:34.320 --> 00:41:41.760
that AI factories are now part of a country's&nbsp;
infrastructure which is the reason why you see&nbsp;&nbsp;

00:41:41.760 --> 00:41:48.880
me running around the world talking to heads of&nbsp;
state, because they all want to have AI factories.&nbsp;&nbsp;

00:41:48.880 --> 00:41:53.920
They all want AI to be part of their&nbsp;
infrastructure. They want AI to be a&nbsp;&nbsp;

00:41:53.920 --> 00:42:00.720
growth manufacturing industry for them.
And this is genuinely profound.&nbsp;

00:42:00.720 --> 00:42:05.120
And I think we're talking about,&nbsp;
as a result of all that, a new&nbsp;&nbsp;

00:42:05.120 --> 00:42:11.040
industrial revolution because every single&nbsp;
industry is affected, and a new industry.&nbsp;

00:42:11.040 --> 00:42:18.560
Just as electricity became a new industry.
At first when it was described as a&nbsp;&nbsp;

00:42:18.560 --> 00:42:23.440
technology and demonstrated as a technology.
It was understood as a technology, but then we&nbsp;&nbsp;

00:42:23.440 --> 00:42:28.000
understood that it's also a large industry.
Then there's the information in industry.&nbsp;

00:42:28.000 --> 00:42:33.040
Which we now know as the internet. And both of&nbsp;
them, because it affected so many industries,&nbsp;&nbsp;

00:42:33.040 --> 00:42:37.120
became part of infrastructure.&nbsp;
We now have a new industry.&nbsp;

00:42:37.120 --> 00:42:42.080
An AI industry and it's now part of the&nbsp;
new infrastructure called intelligence&nbsp;&nbsp;

00:42:42.080 --> 00:42:47.920
infrastructure. Every country, every society,&nbsp;
every company will depend on it. And you could&nbsp;&nbsp;

00:42:47.920 --> 00:42:53.280
see its scale. This is one that's being talked&nbsp;
about a lot. This is Stargate. This doesn't look&nbsp;&nbsp;

00:42:53.280 --> 00:43:01.200
like a data center. It looks like a factory. This&nbsp;
is 1 GW. It will hold about 500,000 GPU dies.&nbsp;

00:43:01.200 --> 00:43:06.080
And produce an enormous amount of&nbsp;
intelligence that could be used by everybody.&nbsp;

00:43:06.080 --> 00:43:13.760
Well, Europe has now awakened to the&nbsp;
importance of these AI factories,&nbsp;&nbsp;

00:43:13.760 --> 00:43:19.600
the importance of the AI infrastructure, and&nbsp;
I'm so delighted to see so much activity here.&nbsp;

00:43:19.600 --> 00:43:24.720
This is European telcos building&nbsp;
AI infrastructure with NVIDIA.&nbsp;

00:43:26.160 --> 00:43:32.240
This is the European cloud service providers&nbsp;
building AI infrastructure with NVIDIA.&nbsp;

00:43:32.240 --> 00:43:37.040
And this is the European Supercomputing&nbsp;
centers, building next generation AI&nbsp;&nbsp;

00:43:37.040 --> 00:43:42.480
supercomputers and infrastructure with NVIDIA.&nbsp;
And this is just the beginning. This is.&nbsp;

00:43:42.480 --> 00:43:48.160
In addition to what will come in the&nbsp;
public clouds. This is in addition to&nbsp;&nbsp;

00:43:48.160 --> 00:43:55.360
the public clouds. So indigenous-built AI&nbsp;
infrastructure here in Europe by European&nbsp;&nbsp;

00:43:55.360 --> 00:44:01.920
companies for the European market.
And then there's 20 more being planned.&nbsp;

00:44:01.920 --> 00:44:07.680
20 more AI factories and several&nbsp;
that are gigawatt factories.&nbsp;

00:44:07.680 --> 00:44:15.520
In total in just two years we will&nbsp;
increase the amount of AI computing&nbsp;&nbsp;

00:44:15.520 --> 00:44:22.800
capacity in Europe by a factor of ten.
And so the researchers, the startups:&nbsp;&nbsp;

00:44:22.800 --> 00:44:37.200
your AI shortage, your GPU shortage will be&nbsp;
resolved for you soon. It's coming for you.&nbsp;

00:44:37.200 --> 00:44:44.640
Now we're partnering with each country to&nbsp;
develop their ecosystem. And so we're building AI&nbsp;&nbsp;

00:44:44.640 --> 00:44:51.120
technology centers in seven different countries.
And the goal of these AI technology centers&nbsp;&nbsp;

00:44:51.120 --> 00:44:57.360
is one, to do collaborative research
To work with the startups and also to build&nbsp;&nbsp;

00:44:57.360 --> 00:45:03.760
the ecosystem. Let me show you what an ecosystem&nbsp;
looks like in the Uk. I was just there yesterday.&nbsp;

00:45:03.760 --> 00:45:08.800
The ecosystems are built on top of&nbsp;
the NVIDIA stack. So for example.&nbsp;

00:45:08.800 --> 00:45:12.400
Every single NVIDIA. As you know,&nbsp;
NVIDIA is the only AI architecture&nbsp;&nbsp;

00:45:12.400 --> 00:45:16.960
that's available in every cloud.
It's the only computing architecture&nbsp;&nbsp;

00:45:16.960 --> 00:45:24.240
aside from X86 that's available everywhere.
We partner with every cloud service provider.&nbsp;

00:45:24.240 --> 00:45:29.520
We accelerate applications from the most important&nbsp;
software developers and in the world, Siemens&nbsp;&nbsp;

00:45:29.520 --> 00:45:36.880
here in Europe, Cadence, Red Hat, Servicenow.
We've reinvented the computing stack. As you know,&nbsp;&nbsp;

00:45:36.880 --> 00:45:41.200
computing is not just a computer, but&nbsp;
it's compute, networking, and storage.&nbsp;&nbsp;

00:45:41.200 --> 00:45:46.640
Each one of those layers, each one of those stacks&nbsp;
has been reinvented. Great partnership with Cisco,&nbsp;&nbsp;

00:45:46.640 --> 00:45:51.680
who announced a brand new model yesterday&nbsp;
at their conference based on NVIDIA. Dell.&nbsp;&nbsp;

00:45:51.680 --> 00:45:57.200
Great partnerships. Netapp. Nuts. A&nbsp;
whole bunch of great partnerships.&nbsp;

00:45:57.200 --> 00:46:02.480
As I mentioned earlier, the way you develop&nbsp;
software has been fundamentally changed.&nbsp;

00:46:02.480 --> 00:46:08.480
It's no longer just write C programs,&nbsp;
compile C programs, deliver C programs.&nbsp;

00:46:08.480 --> 00:46:16.880
It's now Devops, MLOps, AI Ops. So that&nbsp;
entire ecosystem is being reinvented and&nbsp;&nbsp;

00:46:16.880 --> 00:46:21.040
we have ecosystem partners everywhere&nbsp;
and then of course solution integrators&nbsp;&nbsp;

00:46:21.040 --> 00:46:26.240
and providers who could then help every&nbsp;
company integrate these capabilities.&nbsp;

00:46:26.240 --> 00:46:31.200
Well, here in the UK we have special companies&nbsp;
that we work with, really terrific companies&nbsp;&nbsp;

00:46:31.200 --> 00:46:40.400
from researchers to developers, to partners to&nbsp;
help us upskill the local economy and upskill&nbsp;&nbsp;

00:46:40.400 --> 00:46:45.920
the local talent, enterprises that consume the&nbsp;
technology, and of course cloud service providers.&nbsp;

00:46:45.920 --> 00:46:50.560
We have great partners in the UK,&nbsp;
We have great partners in Germany.&nbsp;

00:46:50.560 --> 00:46:57.760
Incredible, incredible partnerships in Germany. We&nbsp;
have partnerships in Italy, and we of course have&nbsp;&nbsp;

00:46:57.760 --> 00:47:18.480
amazing partnerships here in France.
That's right, go France.&nbsp;

00:47:26.000 --> 00:47:32.080
President Macron is going to be here later on.&nbsp;
We're going to talk about some new announcements.&nbsp;&nbsp;

00:47:32.080 --> 00:47:42.240
So we have to show some enthusiasm for AI.
Okay. Yeah. There you go. Show&nbsp;&nbsp;

00:47:42.240 --> 00:47:46.400
him some enthusiasm.
So great partnerships.&nbsp;

00:47:46.400 --> 00:47:51.760
Here in France, one particular one I want to&nbsp;
highlight is our partnership with Schneider.&nbsp;&nbsp;

00:47:51.760 --> 00:47:58.560
Building, even building these AI factories,&nbsp;
we build them digitally now. We design them&nbsp;&nbsp;

00:47:58.560 --> 00:48:03.920
digitally, we build them digitally, we operate&nbsp;
them or optimize them digitally, and we will&nbsp;&nbsp;

00:48:03.920 --> 00:48:09.520
even eventually optimize them and operate&nbsp;
them completely digitally in a digital twin.&nbsp;

00:48:10.320 --> 00:48:15.920
AI factories are so expensive.
$50 billion sometimes.&nbsp;

00:48:15.920 --> 00:48:20.880
$100 billion in the future.
If the utilization of that factory is&nbsp;&nbsp;

00:48:20.880 --> 00:48:27.600
not at its fullest, the cost to the factory&nbsp;
owner is going to be incredible. And so we&nbsp;&nbsp;

00:48:27.600 --> 00:48:33.280
need to digitalize and use AI wherever we can.&nbsp;
Put everything into Omniverse so that we have&nbsp;&nbsp;

00:48:33.280 --> 00:48:39.520
direct and constant telemetry. We have a great&nbsp;
partnership here that we're announcing today.&nbsp;

00:48:39.520 --> 00:48:49.120
Young company , a CEO. I really like, and&nbsp;
he's trying to build a European AI company.&nbsp;

00:48:49.120 --> 00:49:01.840
The name of the company is Mistral. Today&nbsp;
we're announcing that we're going to build&nbsp;&nbsp;

00:49:01.840 --> 00:49:08.160
an in AI cloud together here to deliver their&nbsp;
models as well as deliver AI applications for&nbsp;&nbsp;

00:49:08.160 --> 00:49:12.480
the ecosystem of other AI startups so&nbsp;
that they can use the Mistral models,&nbsp;&nbsp;

00:49:12.480 --> 00:49:17.760
or any model that they like. And so with Mistral&nbsp;
we are going to be partnering to build a very&nbsp;&nbsp;

00:49:17.760 --> 00:49:32.240
sizable AI cloud here. And we'll, we'll talk about&nbsp;
more of it later on today with President Macron.&nbsp;

00:49:32.240 --> 00:49:37.840
AI technology is moving at light speed.
And what I'm showing you here, proprietary&nbsp;&nbsp;

00:49:37.840 --> 00:49:43.840
models on the left moving at light speed.
However, the open models are also moving&nbsp;&nbsp;

00:49:43.840 --> 00:49:53.280
at light speed, only a few months behind.
Whether it's Mistral, Llama, DeepSeek R1,&nbsp;&nbsp;

00:49:53.280 --> 00:50:00.880
R2 coming, Qwen. These models are all exceptional.
Every single one of them is exceptional. And&nbsp;&nbsp;

00:50:00.880 --> 00:50:04.720
so we've dedicated ourselves over the&nbsp;
last several years to apply some of the&nbsp;&nbsp;

00:50:04.720 --> 00:50:13.040
world's best AI researchers to make those AI&nbsp;
models even better and we call that Nemotron.&nbsp;

00:50:13.040 --> 00:50:18.720
Basically, what we do is we take the models&nbsp;
that are open-sourced, and of course they're&nbsp;&nbsp;

00:50:18.720 --> 00:50:24.880
all built on NVIDIA anyhow, and we take those&nbsp;
open-sourced models, we then post-train it.&nbsp;

00:50:24.880 --> 00:50:31.600
We might do neural architecture search.&nbsp;
We might do neural architecture search,&nbsp;&nbsp;

00:50:31.600 --> 00:50:37.440
provided with even better data.
Use reinforcement learning techniques, enhance&nbsp;&nbsp;

00:50:37.440 --> 00:50:43.520
those models, give it reasoning capabilities.
Extend the context so that it could learn and&nbsp;&nbsp;

00:50:43.520 --> 00:50:49.360
read more before it interacts with you.
Most of these models have relatively&nbsp;&nbsp;

00:50:49.360 --> 00:50:54.000
short context, and we wanted to have enormous&nbsp;
context capability because we want to use it in&nbsp;&nbsp;

00:50:54.000 --> 00:50:58.720
enterprise applications where the conversation&nbsp;
we want to have with it is not available on the&nbsp;&nbsp;

00:50:58.720 --> 00:51:03.120
internet. It's available in our company, and so&nbsp;
we have to load it up with an enormous amount&nbsp;&nbsp;

00:51:03.120 --> 00:51:09.360
of context. All of that capability is then&nbsp;
packaged together into a downloadable NIM.&nbsp;&nbsp;

00:51:09.360 --> 00:51:16.800
You could come to NVIDIA's website and literally&nbsp;
download an API, a state of the art AI model.&nbsp;

00:51:16.800 --> 00:51:22.320
Put it anywhere you like and&nbsp;
we improve it tremendously.&nbsp;

00:51:22.320 --> 00:51:32.320
This is an example of Nemotron improvement over&nbsp;
Llama. So Llama 8B, 70B, 405B improved by our&nbsp;&nbsp;

00:51:32.320 --> 00:51:38.400
post-training capability, extension of reasoning&nbsp;
capability, all the data that we provide. We&nbsp;&nbsp;

00:51:38.400 --> 00:51:44.320
enhanced it tremendously. We're going to do this&nbsp;
generation after generation after generation. And&nbsp;&nbsp;

00:51:44.320 --> 00:51:50.000
so for all of you who use Nemotron, you will&nbsp;
know that there's a whole slew of other models&nbsp;&nbsp;

00:51:50.000 --> 00:51:54.480
in the future, and they're open anyway, so if&nbsp;
you would like to start from the open model,&nbsp;&nbsp;

00:51:54.480 --> 00:51:58.560
that's terrific. If you'd like to start with&nbsp;
the Nemotron model, that's terrific. And the&nbsp;&nbsp;

00:51:58.560 --> 00:52:05.200
Nemotron models, the performance is excellent.&nbsp;
In benchmarks after benchmarks after benchmarks,&nbsp;&nbsp;

00:52:05.200 --> 00:52:11.920
Nemotron performance has top of the leaderboard&nbsp;
all over the place. And so now you know that you&nbsp;&nbsp;

00:52:11.920 --> 00:52:18.800
have access to an enhanced open model that is&nbsp;
still open, that is top of the leader charts.&nbsp;

00:52:18.800 --> 00:52:22.240
And you know that NVIDIA is dedicated&nbsp;
to this. And so I will do this for&nbsp;&nbsp;

00:52:22.240 --> 00:52:32.640
as long as I shall live. Okay.
This strategy is so good. This&nbsp;&nbsp;

00:52:32.640 --> 00:52:39.520
strategy is so good that the regional model&nbsp;
makers, the model builders across Europe,&nbsp;&nbsp;

00:52:39.520 --> 00:52:45.440
have now recognized how wonderful the strategy is&nbsp;
and we're partnering together to adapt, enhance&nbsp;&nbsp;

00:52:45.440 --> 00:52:53.760
each one of those models for regional languages.
Your data belongs to you. Your data belongs&nbsp;&nbsp;

00:52:53.760 --> 00:52:57.440
to you. It is the history of your&nbsp;
people, the knowledge of your people,&nbsp;&nbsp;

00:52:57.440 --> 00:53:03.520
the culture of your people. It belongs to you.
And for many companies, in the case of NVIDIA,&nbsp;&nbsp;

00:53:03.520 --> 00:53:12.480
our data is largely inside. 33 years of data.
I was looking up this morning Siemens 180 years of&nbsp;&nbsp;

00:53:12.480 --> 00:53:23.280
data. Some of it written down on papyrus. Roland&nbsp;
Bush is here. I thought I'd pick on Roland Bush,&nbsp;&nbsp;

00:53:23.280 --> 00:53:28.960
my good friend. And so you'll have to&nbsp;
digitize that before the AI can learn.&nbsp;

00:53:28.960 --> 00:53:35.840
And so the data belongs to you. You should use&nbsp;
that data. Use an open model like Nemotron and&nbsp;&nbsp;

00:53:35.840 --> 00:53:41.040
all the tool suites that we provide so&nbsp;
that you can enhance it for your own use.&nbsp;

00:53:41.040 --> 00:53:45.520
We're also announcing that we have a great&nbsp;
partnership with Perplexity. Perplexity as a&nbsp;&nbsp;

00:53:45.520 --> 00:53:49.840
reasoning search engine.
Yep.&nbsp;&nbsp;

00:53:53.600 --> 00:53:59.520
The three models I use, I use ChatGPT, Gemini Pro,&nbsp;
and Perplexity, and these three models are used&nbsp;&nbsp;

00:53:59.520 --> 00:54:04.640
interchangeably. And Perplexity is fantastic.
We're announcing today that Perplexity will&nbsp;&nbsp;

00:54:04.640 --> 00:54:11.600
take these regional models and connect it&nbsp;
right into Perplexity so that you could now&nbsp;&nbsp;

00:54:11.600 --> 00:54:18.400
ask and get questions in the language, in the&nbsp;
culture and the sensibility of your country.&nbsp;

00:54:18.400 --> 00:54:33.600
Okay, so Perplexity regional models.
Agentic AI. Agentic AI. Agents is a very big deal.&nbsp;

00:54:33.600 --> 00:54:39.840
You know, in the beginning with pre-trained&nbsp;
models, people said, but it hallucinates. It&nbsp;&nbsp;

00:54:39.840 --> 00:54:46.720
makes things up. You're absolutely right.
It doesn't have access to the latest news&nbsp;&nbsp;

00:54:46.720 --> 00:54:50.960
and data information.
Absolutely right.&nbsp;

00:54:50.960 --> 00:54:57.440
It gives up without reasoning through problems.&nbsp;
It's as if every single answer has to be memorized&nbsp;&nbsp;

00:54:57.440 --> 00:55:02.880
from the past. You're absolutely right.
All of those things, you know, why is it&nbsp;&nbsp;

00:55:02.880 --> 00:55:08.400
trying to figure out how to add or count the&nbsp;
count numbers and add numbers? Why doesn't&nbsp;&nbsp;

00:55:08.400 --> 00:55:14.960
it use a calculator? You're absolutely right.
And so all of those capabilities associated with&nbsp;&nbsp;

00:55:14.960 --> 00:55:20.560
intelligence, everybody was able to criticize,&nbsp;
but it was absolutely right because everybody&nbsp;&nbsp;

00:55:20.560 --> 00:55:27.040
largely understands how intelligence works.
But those technologies were being built all around&nbsp;&nbsp;

00:55:27.040 --> 00:55:34.960
the world and they were all coming together, from&nbsp;
retrieval augmented generation, to web search,&nbsp;&nbsp;

00:55:34.960 --> 00:55:42.000
to multimodal understanding so that you can read&nbsp;
PDFs, go to a website, look at the images and the&nbsp;&nbsp;

00:55:42.000 --> 00:55:50.080
words, listen to the videos, watch the videos.
And then take all of that understanding into&nbsp;&nbsp;

00:55:50.080 --> 00:55:55.440
your context. You could also now understand,&nbsp;
of course, a prompt from almost anything.&nbsp;

00:55:55.440 --> 00:56:00.320
You could even say, I'm going to ask you&nbsp;
a question, but start from this image. I&nbsp;&nbsp;

00:56:00.320 --> 00:56:05.600
could say start from this text.
Before you answer the question&nbsp;&nbsp;

00:56:05.600 --> 00:56:09.360
or do what I ask you to do.
It then goes off and reasons&nbsp;&nbsp;

00:56:09.360 --> 00:56:14.240
and plans and evaluates itself.
All of those capabilities are now&nbsp;&nbsp;

00:56:14.240 --> 00:56:19.520
integrated and you can see it coming out into&nbsp;
the marketplace all over the place. Agentic AI&nbsp;&nbsp;

00:56:19.520 --> 00:56:28.640
is real. Agentic AI is a giant step function&nbsp;
from one-shot AI. One-shot AI was necessary to&nbsp;&nbsp;

00:56:28.640 --> 00:56:34.240
lay the foundation so that we can teach the&nbsp;
agents how to be agents. You need some basic&nbsp;&nbsp;

00:56:34.240 --> 00:56:40.320
understanding of knowledge and basic understanding&nbsp;
of reasoning to even be able to be teachable. And&nbsp;&nbsp;

00:56:40.320 --> 00:56:48.800
so pre-training is about the teachability of AI.
Post-training reinforcement learning, supervised&nbsp;&nbsp;

00:56:48.800 --> 00:56:56.480
learning, human demonstration,&nbsp;
context provision, generative AI.&nbsp;

00:56:56.480 --> 00:57:01.040
All of that is coming together to&nbsp;
formulate what is now Agentic AI.&nbsp;

00:57:01.040 --> 00:57:06.000
Let's take a look at one example. Let me show&nbsp;
you something. It's built on Perplexity and&nbsp;&nbsp;

00:57:06.000 --> 00:57:09.160
it's super cool.
[video]&nbsp;

00:57:10.560 --> 00:57:17.600
AI agents are digital assistants. Based on&nbsp;
a prompt, they reason through and break down&nbsp;&nbsp;

00:57:17.600 --> 00:57:23.920
problems into multi-step plans. They use&nbsp;
the proper tools, work with other agents,&nbsp;&nbsp;

00:57:23.920 --> 00:57:31.920
and use context from memory to properly execute&nbsp;
the job on NVIDIA accelerated systems. It starts&nbsp;&nbsp;

00:57:31.920 --> 00:57:38.720
with a simple prompt. Let's ask Perplexity&nbsp;
to help start a food truck in Paris. First,&nbsp;&nbsp;

00:57:38.720 --> 00:57:46.400
the perplexity agent reasons through the&nbsp;
prompt and forms a plan. Then calls other&nbsp;&nbsp;

00:57:46.400 --> 00:57:52.640
agents to help tackle each step using&nbsp;
many tools. The market researcher reads&nbsp;&nbsp;

00:57:52.640 --> 00:57:58.560
reviews and reports to uncover trends&nbsp;
and analyze the competitive market.&nbsp;&nbsp;

00:57:59.200 --> 00:58:06.160
Based on this research, a concept designer&nbsp;
explores local ingredients and proposes a menu&nbsp;&nbsp;

00:58:06.160 --> 00:58:15.840
complete with prep time estimates. It researches&nbsp;
palettes and generates a brand identity. Then the&nbsp;&nbsp;

00:58:15.840 --> 00:58:23.840
financial planner uses Monte Carlo simulations to&nbsp;
forecast profitability and growth trajectory. An&nbsp;&nbsp;

00:58:23.840 --> 00:58:30.800
operations planner builds a launch timeline with&nbsp;
every detail, from buying equipment to acquiring&nbsp;&nbsp;

00:58:30.800 --> 00:58:37.040
the right permits. The marketing specialist&nbsp;
builds a launch plan with a social media&nbsp;&nbsp;

00:58:37.040 --> 00:58:46.240
campaign,and even codes an interactive website&nbsp;
including a map, menu, and online ordering. Each&nbsp;&nbsp;

00:58:46.240 --> 00:58:55.280
agent's work comes together in a final package&nbsp;
proposal, and it all started from a single prompt.

00:59:02.400 --> 00:59:10.320
One prompt, one prompt like that in the original&nbsp;
chatbot would have generated a few hundred tokens,&nbsp;&nbsp;

00:59:10.320 --> 00:59:15.600
but now with that one single prompt&nbsp;
into an agent to solve a problem,&nbsp;&nbsp;

00:59:15.600 --> 00:59:21.600
it must have generated 10,000 times more tokens.
This is the reason why Grace Blackwell is&nbsp;&nbsp;

00:59:21.600 --> 00:59:27.040
necessary. This is the reason why we need&nbsp;
performance and the systems to be so much&nbsp;&nbsp;

00:59:27.040 --> 00:59:34.240
more performant generationally. Well, this is how&nbsp;
Perplexity builds their agents. Every company will&nbsp;&nbsp;

00:59:34.240 --> 00:59:40.480
have to build their own agents. It's is terrific,&nbsp;
you're going to be hiring agents from OpenAI and&nbsp;

00:59:40.480 --> 00:59:47.360
Gemini and Microsoft CoPilot and&nbsp;
Perplexity and Mistral and they'll&nbsp;&nbsp;

00:59:47.360 --> 00:59:53.280
be agents that are built for you.
And they might help you plan a vacation or&nbsp;&nbsp;

00:59:53.280 --> 00:59:59.680
you know, go do some research, so on, so forth.
However, if you want to build a company, you're&nbsp;&nbsp;

00:59:59.680 --> 01:00:05.680
going to need specialized agents with specialized&nbsp;
tools and using specialized tools and specialized&nbsp;&nbsp;

01:00:05.680 --> 01:00:11.280
skills. And so the question is, how do you build&nbsp;
those agents? And so we created a platform for&nbsp;&nbsp;

01:00:11.280 --> 01:00:15.840
you. We created a framework, and a set of tools&nbsp;
that you can use and a whole bunch of partners&nbsp;&nbsp;

01:00:15.840 --> 01:00:21.360
to help you do it. It starts with on the very&nbsp;
bottom, on the very bottom, the ability to have&nbsp;&nbsp;

01:00:21.360 --> 01:00:26.400
reasoning models that I spoke about.
NVIDIA’s Nemotron reasoning Large&nbsp;&nbsp;

01:00:26.400 --> 01:00:30.720
Language models are world-class.
We have NeMo Retriever, which is&nbsp;&nbsp;

01:00:30.720 --> 01:00:38.160
a multimodal search engine, semantic search&nbsp;
engine, incredible. And we built a blueprint,&nbsp;&nbsp;

01:00:38.160 --> 01:00:46.800
a demonstration that is operational, that is&nbsp;
essentially a general agent. We call it AI-Q.&nbsp;

01:00:46.800 --> 01:00:53.520
And on top we have a suite of tools&nbsp;
that allows you to onboard an agent,&nbsp;&nbsp;

01:00:53.520 --> 01:01:02.160
a general agent, curate data to teach it,&nbsp;
evaluate it, guardrail it, supervised train it,&nbsp;&nbsp;

01:01:02.160 --> 01:01:09.200
use reinforcement learning all the way to&nbsp;
deployment, keep it secure, keep it safe.&nbsp;

01:01:09.200 --> 01:01:14.560
That suite of toolkits is integrated, those&nbsp;
libraries are integrated, into the AI ops&nbsp;&nbsp;

01:01:14.560 --> 01:01:19.280
ecosystem. You can come and download it from&nbsp;
our website yourself as well, but it's largely&nbsp;&nbsp;

01:01:19.280 --> 01:01:26.560
integrated into AI ops ecosystem. From that,&nbsp;
you could create your own special agents.&nbsp;

01:01:26.560 --> 01:01:32.800
Many companies are doing this. This is Cisco.&nbsp;
They announced it yesterday. We're building AI&nbsp;&nbsp;

01:01:32.800 --> 01:01:38.080
platforms together for security.
Now look at this.&nbsp;

01:01:38.080 --> 01:01:43.920
AI agents, and not one model that does all of&nbsp;
these amazing things, are a collection or a&nbsp;&nbsp;

01:01:43.920 --> 01:01:49.600
system of models. It's a system of large&nbsp;
language models. Some of them are optimized&nbsp;&nbsp;

01:01:49.600 --> 01:01:55.360
for certain types of things. Retrieval, as I&nbsp;
mentioned, performing skills, using a computer.&nbsp;

01:01:55.360 --> 01:02:02.320
You don't want to bundle all of that stuff up&nbsp;
into one giant mass of AI, but you break it up&nbsp;&nbsp;

01:02:02.320 --> 01:02:08.720
into small things that you could then deploy CICD&nbsp;
over time. This is an example of Cisco's. Now the&nbsp;&nbsp;

01:02:08.720 --> 01:02:13.280
question is how do you now deploy this?
Because as I mentioned earlier,&nbsp;&nbsp;

01:02:13.280 --> 01:02:18.720
there are public clouds where NVIDIA’s&nbsp;
compute is, there are regional clouds,&nbsp;&nbsp;

01:02:18.720 --> 01:02:26.560
we call them NCPs here, for example Mistral.
You might have something that is a private cloud,&nbsp;&nbsp;

01:02:26.560 --> 01:02:30.480
because of your security requirements&nbsp;
and your data privacy requirements.&nbsp;

01:02:30.480 --> 01:02:33.520
You might even decide that you&nbsp;
have something on your desk.&nbsp;

01:02:33.520 --> 01:02:37.920
And so the question is, how do you run all of&nbsp;
these, and sometimes they’re running in different&nbsp;&nbsp;

01:02:37.920 --> 01:02:42.320
places because these are all microservices.
These are AIs that could talk to each other,&nbsp;&nbsp;

01:02:42.320 --> 01:02:46.800
they could obviously talk to each other over&nbsp;
networking. And so how do you deploy all of&nbsp;&nbsp;

01:02:46.800 --> 01:02:52.800
these microservices? Well, we now have a great&nbsp;
system. I'm so happy to announce this for you.&nbsp;

01:02:52.800 --> 01:02:58.560
This is called our DGX Lepton. DGX&nbsp;
Lepton. What you're looking at here&nbsp;&nbsp;

01:02:58.560 --> 01:03:04.160
is a whole bunch of different clouds. Here's&nbsp;
the Lambda cloud, the AWS cloud, you know,&nbsp;&nbsp;

01:03:04.160 --> 01:03:09.360
here's your own developer’s machine. Your&nbsp;
own system could be a DGX station. Nebius,&nbsp;&nbsp;

01:03:09.360 --> 01:03:15.440
Yoda, and Scale. It could be AWS, it could&nbsp;
be GCP. NVIDIA’s architecture is everywhere.&nbsp;

01:03:15.440 --> 01:03:22.000
And so you could decide where you would like&nbsp;
to run your models. You deploy it using one&nbsp;&nbsp;

01:03:22.000 --> 01:03:28.080
super cloud. So it's a cloud of clouds. Once&nbsp;
you get it to work, once you get these NIMs&nbsp;&nbsp;

01:03:28.080 --> 01:03:37.920
deployed into Lepton, it will go and be hosted&nbsp;
and run on the various clouds that you decide.&nbsp;&nbsp;

01:03:37.920 --> 01:03:44.480
One model architecture, one deployment and you&nbsp;
can run it everywhere. You can even run it on&nbsp;&nbsp;

01:03:44.480 --> 01:03:53.600
this little tiny machine here.
You know, this DGX Spark.&nbsp;

01:04:00.240 --> 01:04:06.960
Is it cafe time?
Look at this&nbsp;

01:04:14.640 --> 01:04:21.440
This lift is 2000 horsepower.
This is my favorite little machine.&nbsp;&nbsp;

01:04:21.440 --> 01:04:28.960
DGX Spark, the first…the AI supercomputer.&nbsp;
We built an AI supercomputer in 2016. It's&nbsp;&nbsp;

01:04:28.960 --> 01:04:34.480
called the DGX-1. It was the first version.&nbsp;
everything that I've been talking about.&nbsp;&nbsp;

01:04:34.480 --> 01:04:42.480
Eight Volta GPUs connected with NVLink.
It took us billions of dollars to build.&nbsp;&nbsp;

01:04:42.480 --> 01:04:50.320
And on the day we announced it, DGX-1, there&nbsp;
were customers, no interest, no applause.&nbsp;

01:04:50.320 --> 01:04:55.600
100% confusion
Why would somebody build a computer like that?&nbsp;

01:04:55.600 --> 01:05:03.920
Does it run windows? Nope.
And so we built it anyway. Well,&nbsp;&nbsp;

01:05:03.920 --> 01:05:10.720
thankfully, a young company, a startup, a&nbsp;
nonprofit startup in San Francisco was so&nbsp;&nbsp;

01:05:10.720 --> 01:05:15.920
delighted to see the computer. They said, can we&nbsp;
have one? And I thought, oh my gosh, we sold one,&nbsp;&nbsp;

01:05:15.920 --> 01:05:24.480
but then I discovered it was a non-profit.
But I put a computer, put a DGX-1 in my car,&nbsp;&nbsp;

01:05:24.480 --> 01:05:30.800
and I drove it up to San Francisco.&nbsp;
And the name of that company is OpenAI.&nbsp;

01:05:37.200 --> 01:05:43.600
I don't know what the life lesson is there.&nbsp;
There are a lot of non profits. You know,&nbsp;&nbsp;

01:05:43.600 --> 01:05:49.360
so next time, next time maybe the lesson is this:&nbsp;
if a developer reaches out to you a needs a GPU,&nbsp;&nbsp;

01:05:49.360 --> 01:06:00.160
the answer is yes. And so that's right.
So imagine, you have Lepton. It's in your&nbsp;&nbsp;

01:06:00.160 --> 01:06:06.880
browser and you have this this helm chart. An AI&nbsp;
agent that you've developed and you want to run&nbsp;&nbsp;

01:06:06.880 --> 01:06:12.000
it here and parts of it you want to run in AWS and&nbsp;
parts of it you want to run you know in a regional&nbsp;&nbsp;

01:06:12.000 --> 01:06:17.920
cloud somewhere. You use Lepton, you deploy&nbsp;
your helm chart, and it magically shows up here.&nbsp;

01:06:17.920 --> 01:06:21.520
Okay, and so if you would like to run it&nbsp;
here until you're done with it and ready&nbsp;&nbsp;

01:06:21.520 --> 01:06:25.920
to deploy it and then deploy it into the&nbsp;
cloud, terrific. But the beautiful thing is&nbsp;&nbsp;

01:06:25.920 --> 01:06:35.040
this architecture is based on Grace Blackwell&nbsp;
GB10 versus GB200, vs GB300, and all of these&nbsp;&nbsp;

01:06:35.040 --> 01:06:40.560
different versions of this architecture is&nbsp;
exactly Grace Blackwell. Now this is amazing.&nbsp;

01:06:40.560 --> 01:06:49.120
So we're doing this for Lepton,&nbsp;
but, next, HuggingFace and NVIDIA&nbsp;&nbsp;

01:06:49.120 --> 01:06:53.360
have connected Lepton together.
And so whenever you're training&nbsp;&nbsp;

01:06:53.360 --> 01:06:58.640
a model on HuggingFace, if you would like to&nbsp;
deploy it into Lepton and directly into Spark,&nbsp;&nbsp;

01:06:58.640 --> 01:07:04.080
no problem. It's just one click. So&nbsp;
whether you're training or inferencing,&nbsp;&nbsp;

01:07:04.080 --> 01:07:10.720
we are now connected to HuggingFace and Lepton&nbsp;
will help you decide where you want to deploy&nbsp;&nbsp;

01:07:10.720 --> 01:07:14.680
it. Let's take a look at it.
[video]&nbsp;

01:07:15.680 --> 01:07:20.640
Developers need easy and reliable access&nbsp;
to compute that keeps up with their work.&nbsp;&nbsp;

01:07:20.640 --> 01:07:26.320
Wherever they are, whatever they're&nbsp;
building. DGX Cloud Lepton provides on&nbsp;&nbsp;

01:07:26.320 --> 01:07:33.040
demand access to a global network of GPUs&nbsp;
across clouds, regions, and partners like&nbsp;&nbsp;

01:07:33.040 --> 01:07:42.160
Yoda and Nebius. Multi cloud GPU clusters are&nbsp;
managed through a single unified interface.&nbsp;&nbsp;

01:07:43.760 --> 01:07:48.400
Provisioning is fast. Developers can&nbsp;
scale up the number of nodes quickly&nbsp;&nbsp;

01:07:48.400 --> 01:07:53.680
without complex setups and start training&nbsp;
right away with pre-integrated tools and&nbsp;&nbsp;

01:07:53.680 --> 01:07:59.920
training-ready infrastructure. Progress is&nbsp;
monitored in real time. GPU performance,&nbsp;&nbsp;

01:07:59.920 --> 01:08:06.640
convergence and throughput are at your fingertips.&nbsp;
You can test your fine-tuned models right within&nbsp;&nbsp;

01:08:06.640 --> 01:08:13.040
the console. DGX cloud Lepton can deploy NIM&nbsp;
endpoints or your models in multiple clouds&nbsp;&nbsp;

01:08:13.040 --> 01:08:21.040
or regions for fast distributed inference. Just&nbsp;
like ride-sharing apps connect riders to drivers,&nbsp;&nbsp;

01:08:21.040 --> 01:08:31.360
DGX Cloud Lepton connects developers to GPU&nbsp;
compute, powering a virtual global AI factory.

01:08:31.360 --> 01:08:39.360
DGX cloud Lepton.
Okay, so that's Cisco. This is the way SAP,&nbsp;&nbsp;

01:08:39.360 --> 01:08:46.240
they're building an AI platform onNVIDIA. Sana&nbsp;
is building an AI business application automation&nbsp;&nbsp;

01:08:46.240 --> 01:08:54.800
on NVIDIA. DeepL is building their language&nbsp;
framework and platform on NVIDIA. AI Photo Room,&nbsp;&nbsp;

01:08:54.800 --> 01:09:02.080
a video editing and AI editing platform is&nbsp;
building their platform on NVIDIA. And this&nbsp;&nbsp;

01:09:02.080 --> 01:09:08.960
is Qodo, I think used to be Qodium, incredible&nbsp;
coding agent built on NVIDIA. And this is Aiola,&nbsp;&nbsp;

01:09:08.960 --> 01:09:17.680
a voice platform built on NVIDIA. And this one&nbsp;
is a clinical trial platform, the world's largest&nbsp;&nbsp;

01:09:17.680 --> 01:09:23.120
automation platform for clinical trials,&nbsp;
built on NVIDIA. And so all of these,&nbsp;&nbsp;

01:09:23.120 --> 01:09:32.400
all of these basically build on the same idea.
NIMs that encapsulate and package up in a virtual&nbsp;&nbsp;

01:09:32.400 --> 01:09:37.760
container that you could deploy anywhere,&nbsp;
the Nemotron large language model or other&nbsp;&nbsp;

01:09:37.760 --> 01:09:44.720
large language models like Mistral or others.
We then integrate libraries that basically covers&nbsp;&nbsp;

01:09:44.720 --> 01:09:52.160
the entire lifecycle of an AI agent. The way you&nbsp;
treat an AI agent is a little bit like a digital&nbsp;&nbsp;

01:09:52.160 --> 01:09:58.240
employee. So your IT department would have to&nbsp;
onboard them, fine-tune them, train them, evaluate&nbsp;&nbsp;

01:09:58.240 --> 01:10:05.120
them, keep them guardrailed, keep them secure&nbsp;
and continuously improve them. And that entire&nbsp;&nbsp;

01:10:05.120 --> 01:10:10.960
framework platform is called NeMo. And all of&nbsp;
that is now being integrated into one application&nbsp;&nbsp;

01:10:10.960 --> 01:10:15.280
framework after another all over the world.
This is just an example of a few of them. And then&nbsp;&nbsp;

01:10:15.280 --> 01:10:20.160
now we make it possible for you to deploy them&nbsp;
anywhere. If you want to deploy it in the cloud,&nbsp;&nbsp;

01:10:20.160 --> 01:10:26.560
you got DGX, you got GB200s in the cloud. If you&nbsp;
want to deploy it on-prem, because you've got&nbsp;&nbsp;

01:10:28.080 --> 01:10:34.240
VMware or Red Hat Linux or Neutonix and you want&nbsp;
to deploy it in your virtual machines on-prem,&nbsp;&nbsp;

01:10:34.240 --> 01:10:36.720
you can do that. If you wanted&nbsp;
to deploy it as a private cloud,&nbsp;&nbsp;

01:10:36.720 --> 01:10:42.560
you could do that. You can deploy it all the way&nbsp;
on your DGX Spark or DGX Station. No problem.&nbsp;

01:10:42.560 --> 01:10:50.240
And so Lepton will help you do all of that.
Let's talk about industrial AI.&nbsp;

01:10:50.240 --> 01:10:59.040
This is one of my favorite moments. This is Roland&nbsp;
Bush. He just, this is a really fun moment. He&nbsp;&nbsp;

01:10:59.040 --> 01:11:08.000
wanted to remind me that neural computers, neural&nbsp;
network computers, were invented in Europe.&nbsp;

01:11:08.000 --> 01:11:15.440
That's this whole slide.
I just, it was such a great moment. This is&nbsp;&nbsp;

01:11:15.440 --> 01:11:26.000
Synapse1. This is incredible, you guys. Synapse1.
This is Synapse1, 1992. It runs neural networks&nbsp;&nbsp;

01:11:26.000 --> 01:11:31.520
8000 times faster than CPUs at that time.
Isn't it incredible? So this&nbsp;&nbsp;

01:11:31.520 --> 01:11:41.120
is the world's AI computer.
And Roland just wants to just. Never forget that,&nbsp;&nbsp;

01:11:41.120 --> 01:11:47.920
Jensen. Never, ever forget that. I said, okay,&nbsp;
all right, I'll tell and I'll even tell everybody.&nbsp;

01:11:47.920 --> 01:11:52.800
Siemens, 1992.
Siemens, 1992. We have a great partnership with&nbsp;&nbsp;

01:11:52.800 --> 01:12:02.960
Siemens and Roland Bush, the CEO, is supercharging&nbsp;
the company so that they could leap, completely&nbsp;&nbsp;

01:12:02.960 --> 01:12:11.280
leap the last, IT industrial revolution and&nbsp;
fuse the industrial capabilities of Europe, the&nbsp;&nbsp;

01:12:11.280 --> 01:12:16.240
industrial capabilities and might of Siemens with&nbsp;
artificial intelligence and create what is called&nbsp;&nbsp;

01:12:16.240 --> 01:12:22.640
the industrial AI revolution. We're partnering&nbsp;
with Siemens on so many different fronts.&nbsp;

01:12:22.640 --> 01:12:30.160
Everything from design to simulation&nbsp;
to digital twins of factories,&nbsp;&nbsp;

01:12:30.160 --> 01:12:36.960
to operations of the AI in the factories.
Everything from end to end. And it just&nbsp;&nbsp;

01:12:36.960 --> 01:12:45.440
reminds us, it reminds me how incredible&nbsp;
the industrial capabilities of Europe are.&nbsp;

01:12:45.440 --> 01:12:50.320
And what an extraordinary opportunity this is&nbsp;
for you. What an extraordinary opportunity.&nbsp;&nbsp;

01:12:50.320 --> 01:12:59.600
Because AI is unlike software. AI is really smart&nbsp;
software. And this smart software can finally&nbsp;&nbsp;

01:12:59.600 --> 01:13:03.760
do something that can revolutionize&nbsp;
the very industries that you serve.&nbsp;

01:13:03.760 --> 01:13:10.640
And so we made a love letter&nbsp;
video if you will. Let's play it.&nbsp;

01:13:10.640 --> 01:13:16.000
[video]
It began here.&nbsp;

01:13:16.000 --> 01:13:23.600
The first industrial revolution.
Watts’ steam engine and the mechanized loom.&nbsp;

01:13:23.600 --> 01:13:28.640
Introduced automation
And the advent of factories.&nbsp;

01:13:28.640 --> 01:13:36.480
And industry was born
The age of electricity.&nbsp;

01:13:36.480 --> 01:13:44.720
Ampere unraveled electromagnetism.
Faraday built the first electric generator.&nbsp;

01:13:44.720 --> 01:13:50.960
And Maxwell laid the foundations
For modern electrical engineering.&nbsp;

01:13:50.960 --> 01:13:58.000
Siemens and Wheatstone’s Dynamo.
The engine of electricity.&nbsp;

01:13:58.000 --> 01:14:04.880
Bringing machine, trains,&nbsp;
factories and cities to life.&nbsp;

01:14:04.880 --> 01:14:14.320
Electrifying the planet.
Igniting modern manufacturing&nbsp;

01:14:14.320 --> 01:14:18.720
And today, born out of the&nbsp;
computing and information age.&nbsp;

01:14:18.720 --> 01:14:24.880
The 4th Industrial Revolution.
The age of AI.&nbsp;

01:14:24.880 --> 01:14:29.840
Reimagining every part of industry.
Across the continent,&nbsp;&nbsp;

01:14:29.840 --> 01:14:35.440
industrial AI is taking hold.
From design to engineering.&nbsp;

01:14:35.440 --> 01:14:43.680
You're blazing new trails toward&nbsp;
understanding and reinvention.&nbsp;

01:14:43.680 --> 01:14:50.480
You brought the physical world into the virtual.
To plan and optimize&nbsp;

01:14:50.480 --> 01:14:56.000
The world's modern factories.
You're building the next frontier.&nbsp;

01:14:56.000 --> 01:15:07.600
Where everything that moves is robotic.
Every car an intelligent autonomous agent.&nbsp;

01:15:07.600 --> 01:15:12.960
And a new collaborative workforce to&nbsp;
help close the global labor shortage gap.&nbsp;

01:15:15.280 --> 01:15:20.880
Developers across the continent&nbsp;
are building every type of robot.&nbsp;

01:15:20.880 --> 01:15:23.600
Teaching them new skills.&nbsp;

01:15:23.600 --> 01:15:32.400
In digital twin worlds and robot gyms.
Preparing them to work alongside us.&nbsp;

01:15:32.400 --> 01:15:38.160
In our factories.
Warehouses&nbsp;

01:15:38.160 --> 01:15:42.000
The operating room.
And at home.&nbsp;

01:15:42.800 --> 01:15:52.640
The 4th industrial revolution is here.
Right where the first began.

01:15:52.640 --> 01:16:01.120
What do you think?
I love that video.&nbsp;&nbsp;

01:16:01.120 --> 01:16:09.840
You made it. That's so great. You made it.
Well, we're working on industrial AI with one&nbsp;&nbsp;

01:16:09.840 --> 01:16:19.600
company after another. This is a BMW. Building&nbsp;
their next generation factory in Omniverse.&nbsp;

01:16:19.600 --> 01:16:30.320
This is…I don't know how to say it. Can somebody&nbsp;
teach me? Sounds good. Exactly. That's exactly&nbsp;&nbsp;

01:16:30.320 --> 01:16:36.560
right. Good job. Good job. That's exactly right.
They're building, of course, their plants,&nbsp;&nbsp;

01:16:36.560 --> 01:16:45.520
digital twins and Omniverse is key. Their&nbsp;
digital twin for warehouse logistics.&nbsp;

01:16:45.520 --> 01:16:52.000
This is Mercedes Benz and their digital twins&nbsp;
of their factories built in Omniverse. This is&nbsp;&nbsp;

01:16:52.000 --> 01:16:59.280
Schaeffer and their digital twin of&nbsp;
their warehouse built in Omniverse.&nbsp;

01:16:59.280 --> 01:17:06.000
This is your train station here in France&nbsp;
building a digital twin of their train stations in&nbsp;&nbsp;

01:17:06.000 --> 01:17:13.360
Omniverse. And this is Toyota building a digital&nbsp;
twin of their warehouse in Omniverse. And when,&nbsp;&nbsp;

01:17:13.360 --> 01:17:19.200
when you build these warehouses and these&nbsp;
factories and Omniverse, then you could design it,&nbsp;&nbsp;

01:17:19.200 --> 01:17:23.360
you can plan it, you can change it.
In green-field it’s wonderful,&nbsp;&nbsp;

01:17:23.360 --> 01:17:28.400
in brown-field it's wonderful. You could simulate&nbsp;
its effectiveness before you go and physically&nbsp;&nbsp;

01:17:28.400 --> 01:17:33.840
lift and move things around to discover it wasn't&nbsp;
optimal. And so the ability to do everything&nbsp;&nbsp;

01:17:33.840 --> 01:17:40.720
digitally in a digital twin is incredible.
But the question is why does the digital twin have&nbsp;&nbsp;

01:17:40.720 --> 01:17:46.320
to be photoreal and why does it have to obey the&nbsp;
laws of physics? The reason for that is because&nbsp;&nbsp;

01:17:46.320 --> 01:17:53.600
we wanted ultimately to be a digital twin where a&nbsp;
robot could learn how to operate as a robot. And&nbsp;&nbsp;

01:17:53.600 --> 01:18:02.240
robots rely on photons for their perception system&nbsp;
and those photons are generated through Omniverse.&nbsp;

01:18:02.240 --> 01:18:07.920
And a robot needs to interact with the physical&nbsp;
world so that it could know whether it's doing&nbsp;&nbsp;

01:18:07.920 --> 01:18:12.400
the right things and learn how to do it&nbsp;
properly. And so these digital twins have&nbsp;&nbsp;

01:18:12.400 --> 01:18:18.320
to look real and behave realistically. Okay,&nbsp;
so that's the reason why Omniverse was built.&nbsp;

01:18:18.320 --> 01:18:24.880
This is fantastic. This is a fusion reactor&nbsp;
digital twin. Incredibly complicated piece of&nbsp;&nbsp;

01:18:24.880 --> 01:18:29.840
instrument as you know, and without AI the next&nbsp;
generation fusion reactor would not be possible.&nbsp;

01:18:30.480 --> 01:18:38.000
We're announcing today that we are going to&nbsp;
build the world's first industrial AI cloud&nbsp;&nbsp;

01:18:38.000 --> 01:18:46.240
here in Europe.
Yep.&nbsp;

01:18:46.240 --> 01:18:52.400
These industrial AI clouds are, yes, a whole&nbsp;
lot of computers in the cloud. However,&nbsp;&nbsp;

01:18:52.400 --> 01:18:58.960
the requirements, its performance, its safety&nbsp;
requirements fundamentally different. And so I'm&nbsp;&nbsp;

01:18:58.960 --> 01:19:04.000
going to tell you a lot more about it on Friday.&nbsp;
I'm only teasing you part of the story today,&nbsp;&nbsp;

01:19:04.000 --> 01:19:09.760
but this industrial cloud will be&nbsp;
used for design and simulation.&nbsp;

01:19:09.760 --> 01:19:14.640
Virtual wind tunnels that you just walk&nbsp;
into. Virtual wind tunnels you just move&nbsp;&nbsp;

01:19:14.640 --> 01:19:20.560
a car into and you see it behave. Open&nbsp;
doors, open windows, change the design.&nbsp;

01:19:20.560 --> 01:19:26.080
All completely in real time. Design in real&nbsp;
time, simulate in a digital wind tunnel,&nbsp;&nbsp;

01:19:26.080 --> 01:19:31.680
digital twin of a wind tunnel in real time,&nbsp;
build it in a factory of a digital factory&nbsp;&nbsp;

01:19:31.680 --> 01:19:40.320
twin in real time. All of this, and the robots&nbsp;
learn how to be great robots and build the&nbsp;&nbsp;

01:19:40.320 --> 01:19:46.720
robots of our future, self driving cars and such.
We already have a tremendous ecosystem here. We've&nbsp;&nbsp;

01:19:46.720 --> 01:19:51.200
been here, as you know, for a very long time.&nbsp;
NVIDIA is 33 years old. The first time we came&nbsp;&nbsp;

01:19:51.200 --> 01:19:57.840
to Europe was during the time when workstations&nbsp;
and the digitalization of products, CAD, the CAD&nbsp;&nbsp;

01:19:57.840 --> 01:20:04.240
revolution started. We were here during the CAE&nbsp;
revolution and now the digital twin revolution.&nbsp;

01:20:04.240 --> 01:20:09.600
Some $2 trillion of ecosystem here in&nbsp;
Europe that we partner with and that have&nbsp;&nbsp;

01:20:09.600 --> 01:20:15.600
the privilege of supporting. What comes out&nbsp;
of that is a new revolution that's happening.&nbsp;

01:20:15.600 --> 01:20:22.240
As you know, everything that moves will be&nbsp;
robotics, everything that moves will be AI-driven,&nbsp;&nbsp;

01:20:22.240 --> 01:20:29.200
and the car is the most obvious next one.
NVIDIA builds the AI supercomputers&nbsp;&nbsp;

01:20:29.200 --> 01:20:32.400
to train the model.
The AI supercomputer&nbsp;&nbsp;

01:20:32.400 --> 01:20:39.680
for Omniverse digital twins. We also build&nbsp;
the AI supercomputers for the robot itself.&nbsp;

01:20:39.680 --> 01:20:46.400
In every single case, whether it's in the cloud&nbsp;
for Omniverse or in the car, we offer the entire&nbsp;&nbsp;

01:20:46.400 --> 01:20:51.440
stack, the computer itself, the operating&nbsp;
system that runs on top of this computer,&nbsp;&nbsp;

01:20:51.440 --> 01:20:58.480
which is different in every single case.
This computer, high-speed, sensor-rich,&nbsp;&nbsp;

01:20:58.480 --> 01:21:04.960
must be functionally safe. In no&nbsp;
circumstance could it fail completely.&nbsp;

01:21:04.960 --> 01:21:09.760
And so the safety requirements are incredibly&nbsp;
high. And now we have an incredible model that&nbsp;&nbsp;

01:21:09.760 --> 01:21:15.360
sits on top of it. This model that sits on top&nbsp;
of it is a transformer model. It's a reasoning&nbsp;&nbsp;

01:21:15.360 --> 01:21:22.960
model. It takes sensor in, you tell it what&nbsp;
you want it to do, and it will drive you there.&nbsp;

01:21:23.680 --> 01:21:30.400
Takes pixels in and it generates path&nbsp;
plans output. So it's a generative AI&nbsp;&nbsp;

01:21:30.400 --> 01:21:34.560
model based on transformers.
Incredible technology&nbsp;

01:21:34.560 --> 01:21:40.640
NVIDIA’s AI team, AV team, is incredible.
This is the only team that I know that has&nbsp;&nbsp;

01:21:40.640 --> 01:21:45.280
won end-to-end self-driving car&nbsp;
challenge at CVPR two years in a&nbsp;&nbsp;

01:21:45.280 --> 01:21:50.960
row. And so they're the winner again this year.&nbsp;
Let's take a look at the video. Yep, thank you.&nbsp;

01:21:50.960 --> 01:21:54.400
[video]
Let's take a look at what they do.&nbsp;&nbsp;

01:21:54.400 --> 01:21:59.600
Like any driver, autonomous vehicles&nbsp;
operate in a world full of unpredictable&nbsp;&nbsp;

01:21:59.600 --> 01:22:08.400
and potentially safety critical scenarios.
NVIDIA Drive built on the Halos safety system.&nbsp;

01:22:08.400 --> 01:22:13.040
Lets developers build safe autonomous&nbsp;
vehicles with diverse software stacks&nbsp;&nbsp;

01:22:13.040 --> 01:22:21.360
and sensors and redundant computers.
It starts with training. Safe AVs need massive&nbsp;&nbsp;

01:22:21.360 --> 01:22:28.320
amounts of diverse data to be able to address&nbsp;
edge cases, but real world data is limited.&nbsp;

01:22:28.320 --> 01:22:33.360
Developers use NVIDIA Omniverse and&nbsp;
Cosmos to reconstruct the real world&nbsp;&nbsp;

01:22:33.360 --> 01:22:41.040
and generate realistic synthetic training&nbsp;
data to bring diversity to the AV model.&nbsp;

01:22:42.720 --> 01:22:46.560
The model can perceive and&nbsp;
reason about its environment.&nbsp;

01:22:46.560 --> 01:22:53.680
Predict future outcomes.
And generate a motion plan.&nbsp;

01:22:53.680 --> 01:22:56.960
And for decision making diversity, an independent&nbsp;&nbsp;

01:22:56.960 --> 01:23:02.800
classical stack runs in parallel.
Guardrails monitors safe performance.&nbsp;

01:23:02.800 --> 01:23:09.280
And in cases of anomalies, calls the&nbsp;
arbitrator to make an emergency stop.&nbsp;

01:23:11.920 --> 01:23:19.360
Further diversity and redundancy are built&nbsp;
into the sensor and compute architecture.&nbsp;

01:23:19.360 --> 01:23:25.760
Each sensor connects to redundant computers,&nbsp;
so even if a sensor or computer fails,&nbsp;&nbsp;

01:23:25.760 --> 01:23:32.480
the vehicle stays safe and operational.
And in the event of a critical failure,&nbsp;&nbsp;

01:23:32.480 --> 01:23:38.800
the system can execute a minimum risk&nbsp;
maneuver like pulling over to the shoulder.&nbsp;

01:23:38.800 --> 01:23:46.000
Safety is foundational to autonomous driving.
NVIDIA Drive lets developers worldwide&nbsp;&nbsp;

01:23:46.000 --> 01:24:01.520
integrate Halos into their own products&nbsp;
to build the next generation of safe AVs.

01:24:01.520 --> 01:24:09.840
A billion cars on the road, 10,000 miles&nbsp;
a year on average, ten trillion miles.&nbsp;

01:24:09.840 --> 01:24:13.760
The future of autonomous&nbsp;
driving is obviously gigantic.&nbsp;

01:24:13.760 --> 01:24:17.600
And it's going to be driven,&nbsp;
it's going to be powered by AI.&nbsp;

01:24:17.600 --> 01:24:24.720
This is, this is the next gigantic opportunity.
And we're working with enormous companies and&nbsp;&nbsp;

01:24:24.720 --> 01:24:27.840
really fantastic companies around&nbsp;
the world to make this possible.&nbsp;

01:24:27.840 --> 01:24:32.240
At the core of everything we do here&nbsp;
with AV is safety and we're really,&nbsp;&nbsp;

01:24:32.240 --> 01:24:36.800
really proud of our Halos system.
It starts with the architecture of the&nbsp;&nbsp;

01:24:36.800 --> 01:24:42.640
chip and then the chip design and the systems&nbsp;
design, the operating systems, the AI models,&nbsp;&nbsp;

01:24:42.640 --> 01:24:49.200
the methodology of developing the software, the&nbsp;
way we test it, everything from the way we train&nbsp;&nbsp;

01:24:49.200 --> 01:24:54.560
the models, the data we provide for the models,&nbsp;
all the way to the way we evaluate the models.&nbsp;&nbsp;

01:24:54.560 --> 01:25:01.760
NVIDIA’s Halos system and our AV safety team&nbsp;
and capabilities are absolutely world-renowned.&nbsp;&nbsp;

01:25:01.760 --> 01:25:07.080
This computer was the first one to be&nbsp;
software-defined, the world's first software&nbsp;&nbsp;

01:25:07.080 --> 01:25:14.720
defined completely 100% software-defined, AI&nbsp;
driven stack for AVs. We've been at this now for&nbsp;&nbsp;

01:25:14.720 --> 01:25:21.120
coming up on ten years. And so this capability&nbsp;
is world-renowned and I'm really proud of it.&nbsp;

01:25:21.120 --> 01:25:24.960
The same thing that's happening for&nbsp;
cars is happening for a new industry.&nbsp;

01:25:24.960 --> 01:25:30.800
As I mentioned earlier, if you&nbsp;
can generate video from prompts.&nbsp;

01:25:30.800 --> 01:25:36.160
AI can perceive, it can reason.
It can generate videos and words&nbsp;&nbsp;

01:25:36.160 --> 01:25:42.160
and images. And just now with cars,&nbsp;
the path, the steering wheel path.&nbsp;

01:25:42.160 --> 01:25:48.160
Why can't it also generate locomotion&nbsp;
abilities and articulation abilities?&nbsp;&nbsp;

01:25:48.160 --> 01:25:54.000
So that fundamental ability for AI to&nbsp;
revolutionize one of the hardest robotics&nbsp;&nbsp;

01:25:54.000 --> 01:26:00.400
problems is around the corner. Humanoid&nbsp;
robots are going to be a thing. We now&nbsp;&nbsp;

01:26:00.400 --> 01:26:04.960
know how to build these things, train&nbsp;
these things, and operate these things.&nbsp;

01:26:04.960 --> 01:26:10.480
Human robotics is going to potentially&nbsp;
be one of the largest industries ever.&nbsp;

01:26:10.480 --> 01:26:15.120
And it requires companies who&nbsp;
know how to manufacture things.&nbsp;

01:26:15.120 --> 01:26:22.720
Manufacture things of extraordinary capabilities.&nbsp;
This speaks of the European countries. So much&nbsp;&nbsp;

01:26:22.720 --> 01:26:27.440
of the world's industries are based here. I&nbsp;
think this is going to be a giant opportunity.&nbsp;

01:26:27.440 --> 01:26:32.320
Well, let's say it's a billion&nbsp;
robots around the world.&nbsp;

01:26:32.320 --> 01:26:38.400
The idea that there'd be a billion robots is a&nbsp;
very sensible thing. Now why hasn't it happened?&nbsp;

01:26:38.400 --> 01:26:44.560
Well, the reason for that is simple.
Today's robots are too hard to program.&nbsp;

01:26:44.560 --> 01:26:49.920
Only the largest companies can afford&nbsp;
to install a robot, get it, to teach it,&nbsp;&nbsp;

01:26:49.920 --> 01:26:56.640
program it to do exactly the right things.
Keep it sufficiently surrounded so that it's safe.&nbsp;

01:26:56.640 --> 01:27:01.840
That's the reason why the world's largest car&nbsp;
companies have robots. They're large enough.&nbsp;&nbsp;

01:27:01.840 --> 01:27:07.760
The work is sufficiently repetitive.
The industry is at a sufficient scale&nbsp;&nbsp;

01:27:07.760 --> 01:27:13.920
that you could deploy robots into those&nbsp;
factories. Almost everybody who is a middle,&nbsp;&nbsp;

01:27:13.920 --> 01:27:21.280
or small and medium companies, or mom and&nbsp;
pop restaurants or stores or warehouses, it's&nbsp;&nbsp;

01:27:21.280 --> 01:27:28.240
impossible to have that programming capability.
Until now. We're going to give you essentially&nbsp;&nbsp;

01:27:28.240 --> 01:27:33.760
robots where you could teach them. They'll&nbsp;
learn from you. Just as we were talking about&nbsp;&nbsp;

01:27:33.760 --> 01:27:40.720
Agentic AI, we now have humanoid AI that can learn&nbsp;
from your teaching using toolkits that are very&nbsp;&nbsp;

01:27:41.440 --> 01:27:47.520
consistent with the NeMo tools I spoke about.
NVIDIA here as well is built in a three-layer&nbsp;&nbsp;

01:27:47.520 --> 01:27:56.720
stack. We build the computer, the Thor&nbsp;
computer dev kit. looks a little bit like this.&nbsp;

01:27:56.720 --> 01:28:03.680
This is a robotic computer, completely&nbsp;
self-contained dev kit sits on your desk.&nbsp;

01:28:03.680 --> 01:28:09.440
These are all the sensors and inside&nbsp;
is a little supercomputer Thor chip.&nbsp;

01:28:09.440 --> 01:28:19.280
Really, really incredible. And these. Yep.&nbsp;
Yeah. I could imagine getting one of these&nbsp;&nbsp;

01:28:19.280 --> 01:28:28.160
inserted like that. Okay, thank you.
So that's the Thor processor. On top&nbsp;&nbsp;

01:28:28.160 --> 01:28:34.720
is an operating system designed for robotics.&nbsp;
And on top of that, transformer models that&nbsp;&nbsp;

01:28:34.720 --> 01:28:46.080
take sensor data and instructions and transforms&nbsp;
them and generates paths and motor controls for&nbsp;&nbsp;

01:28:46.080 --> 01:28:51.760
arm articulation, finger articulation,&nbsp;
and of course your legs articulation.&nbsp;

01:28:51.760 --> 01:28:58.240
Now the big challenge of human robotics is the&nbsp;
amount of data necessary to train. It is very,&nbsp;&nbsp;

01:28:58.240 --> 01:29:03.200
very hard to get. And so the question is, how do&nbsp;
you do that? Well, the way you solve that problem&nbsp;&nbsp;

01:29:03.200 --> 01:29:10.480
is to back in Omniverse, a digital twin world&nbsp;
that obeys the laws of physics. And this is an&nbsp;&nbsp;

01:29:10.480 --> 01:29:18.560
incredible piece of work that we're doing.
Don't do it. Don't&nbsp;&nbsp;

01:29:21.200 --> 01:29:32.080
Oh. My fault
Okay, these are robots.&nbsp;

01:29:32.080 --> 01:29:39.520
We developed computers to train them, computers&nbsp;
to simulate them and the computer that goes&nbsp;&nbsp;

01:29:39.520 --> 01:29:44.560
inside them. There's a whole bunch of human&nbsp;
robotics companies being built around the world.&nbsp;&nbsp;

01:29:44.560 --> 01:29:51.440
They all see the great opportunity to&nbsp;
revolutionize this new device if you will. And&nbsp;&nbsp;

01:29:51.440 --> 01:29:57.600
the progress is going incredibly fast. And the&nbsp;
way that they all learn is they learn in a virtual&nbsp;&nbsp;

01:29:57.600 --> 01:30:02.880
world. and this virtual world has to obey the&nbsp;
laws of physics. And recently we announced a big&nbsp;&nbsp;

01:30:02.880 --> 01:30:09.280
partnership with Disney Research and DeepMind,&nbsp;
and we're going to work together to create the&nbsp;&nbsp;

01:30:09.280 --> 01:30:15.200
world's most sophisticated physics simulation.
And I'm just trying to figure out at this point&nbsp;&nbsp;

01:30:15.200 --> 01:30:28.160
how to go to that slide. Teach me who's with me.&nbsp;
This is what happens when you only rehearse once.&nbsp;

01:30:28.160 --> 01:30:40.000
Okay, so this this. Incredible system This&nbsp;
incredible system is where an AI learns how to&nbsp;&nbsp;

01:30:40.000 --> 01:30:46.288
be an AI. Let me show it to you.
[video]

01:31:24.560 --> 01:31:35.120
We have a special guest.
Your name is Grek.&nbsp;

01:31:35.920 --> 01:31:49.680
Are you a petite garson or petite belle?
Okay. Grek is a little girl.&nbsp;

01:31:49.680 --> 01:31:55.200
Now look at this. Grek learned&nbsp;
how to walk inside Omniverse.&nbsp;

01:31:55.200 --> 01:31:59.840
Obeying the laws of physics.
But inside Omniverse, we created&nbsp;&nbsp;

01:31:59.840 --> 01:32:05.760
hundreds of thousands of scenarios.
Then finally, when Grek learned how&nbsp;&nbsp;

01:32:05.760 --> 01:32:12.880
to operate and walk and manipulate in those&nbsp;
environments on sand and on, you know, on gravel,&nbsp;&nbsp;

01:32:12.880 --> 01:32:19.600
slippery floors, on concrete, on carpet.
Then when it comes, when Greg comes into&nbsp;&nbsp;

01:32:19.600 --> 01:32:25.600
the physical world, the physical world&nbsp;
is just the 100,001 version of the world.&nbsp;&nbsp;

01:32:25.600 --> 01:32:32.240
And so you learn how to walk the&nbsp;
virtual world and look at you now.&nbsp;

01:32:32.240 --> 01:32:54.080
Can you, can you jump? Wow.
Can you dance?&nbsp;

01:32:54.080 --> 01:33:01.200
Well, I think, I think, um. I just want to&nbsp;
let you know I am the keynote presenter.&nbsp;

01:33:01.200 --> 01:33:06.000
So I need you, I need you to behave. I&nbsp;
need you to behave for a few seconds.&nbsp;

01:33:06.000 --> 01:33:11.680
I need you to behave for a few&nbsp;
seconds. Could you sit? Sit.&nbsp;

01:33:11.680 --> 01:33:18.160
Hey, you know what we should do?&nbsp;
Let's take a picture of everybody.&nbsp;

01:33:18.160 --> 01:33:29.760
Yeah. Bam. bam.
Would you like to come home with me?&nbsp;&nbsp;

01:33:29.760 --> 01:33:36.560
Would you like to come home with me? I got.
Yeah, I know. Yeah, I have pets. They&nbsp;&nbsp;

01:33:36.560 --> 01:33:44.960
would like to have you as a pet.
No? No. You're so smart. You're&nbsp;&nbsp;

01:33:44.960 --> 01:33:58.000
so smart. Well. Incredible right?
Grek you are the world's best robot&nbsp;&nbsp;

01:33:58.000 --> 01:34:06.320
and someday we'll all have one like&nbsp;
you and they'll follow us around.&nbsp;

01:34:06.320 --> 01:34:11.600
But if I need a glass of whiskey, you're going&nbsp;
to have to go tell somebody else to go get me a&nbsp;&nbsp;

01:34:11.600 --> 01:34:21.040
glass of whiskey because you have no arms.
Yeah. You're so cute. Okay little girl,&nbsp;&nbsp;

01:34:21.040 --> 01:34:33.760
you stay here for a second. Let's wrap up.
Alright. It's very clear. It's very clear an&nbsp;&nbsp;

01:34:33.760 --> 01:34:40.160
industrial revolution has started. The&nbsp;
next, the next wave of AI has started.&nbsp;

01:34:40.160 --> 01:34:46.400
Grek is a perfect example of what's possible&nbsp;
now with robotics. The technology necessary to&nbsp;&nbsp;

01:34:46.400 --> 01:34:55.760
teach a robot. To manipulate to simulate.
And of course the manifestation of an&nbsp;&nbsp;

01:34:55.760 --> 01:35:02.640
incredible robot is now right in front of us.
We have physical robots and we have information&nbsp;&nbsp;

01:35:02.640 --> 01:35:09.120
robots. We call them agents. So the next wave of&nbsp;
AI has started. It's going to require inference&nbsp;&nbsp;

01:35:09.120 --> 01:35:14.720
workloads to explode, it's basically going to&nbsp;
go exponential. The number of people that are&nbsp;&nbsp;

01:35:14.720 --> 01:35:19.920
using inference has gone from 8 million to 800&nbsp;
million, a hundred times in just a couple of&nbsp;&nbsp;

01:35:19.920 --> 01:35:25.360
years. The number, the amount of prompts that,&nbsp;
the tokens generated, as I mentioned earlier,&nbsp;&nbsp;

01:35:25.360 --> 01:35:31.920
from a few hundred tokens to thousands of tokens.&nbsp;
And of course we use AI even more than ever today.&nbsp;

01:35:31.920 --> 01:35:36.000
So we need a special computer designed&nbsp;
for thinking, designed for reasoning. And&nbsp;&nbsp;

01:35:36.000 --> 01:35:41.360
that's what Blackwell is, a thinking machine.
These Blackwells will go into new types of&nbsp;&nbsp;

01:35:41.360 --> 01:35:47.040
datacenters, essentially AI factories designed&nbsp;
for one thing and one thing only. And these AI&nbsp;&nbsp;

01:35:47.040 --> 01:35:52.400
factories are going to generate tokens, and these&nbsp;
tokens are going to become your food, little Grek.&nbsp;

01:35:52.400 --> 01:35:57.440
I know, I know.
And what's really incredible,&nbsp;&nbsp;

01:35:57.440 --> 01:36:03.120
I'm so happy to see that Europe is going all&nbsp;
in on AI. The amount of AI infrastructure&nbsp;&nbsp;

01:36:03.120 --> 01:36:08.640
being built here will increase by an order&nbsp;
of magnitude in the next couple of years.&nbsp;

01:36:08.640 --> 01:36:16.560
I want to thank all of you for your&nbsp;
partnership. Have a great VivaTech. Thank you.&nbsp;

01:36:16.560 --> 01:36:23.120
Say bye bye. Say bye bye. Take a bunch of&nbsp;
pictures. Take a bunch of pictures. Take a&nbsp;&nbsp;

01:36:23.120 --> 01:36:28.800
bunch of pictures.
Yeah.

